{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install seaborn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OzlHyFA1vAO",
        "outputId": "7d819c99-6c80-46ec-a0a6-fd9777d906bc"
      },
      "id": "3OzlHyFA1vAO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82ce088d-d030-43f8-a2e6-a7e4cee1924f",
      "metadata": {
        "id": "82ce088d-d030-43f8-a2e6-a7e4cee1924f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch import Tensor\n",
        "import math\n",
        "from math import sqrt\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BertTokenizer,\n",
        "    BertModel\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45cRTWUftvRb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45cRTWUftvRb",
        "outputId": "76b39c53-4e10-471f-a140-91d79ce0f748"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory allocated: 0.00 MB\n",
            "GPU memory cached: 0.00 MB\n"
          ]
        }
      ],
      "source": [
        "# Monitor GPU memory usage\n",
        "def print_gpu_memory():\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
        "        print(f\"GPU memory cached: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "# Add these at the start of your script\n",
        "set_seed(42)\n",
        "print_gpu_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8ee03b4-3d55-45f1-8907-a298ba88911c",
      "metadata": {
        "id": "a8ee03b4-3d55-45f1-8907-a298ba88911c"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation=\"relu\"):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        d_ff = d_ff or 4 * d_model\n",
        "        self.attention = attention\n",
        "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
        "\n",
        "    def forward(self, x, attn_mask=None, tau=None, delta=None):\n",
        "        new_x, attn = self.attention(x, x, x, attn_mask=attn_mask, tau=tau, delta=delta)\n",
        "        x = x + self.dropout(new_x)\n",
        "\n",
        "        y = x = self.norm1(x)\n",
        "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
        "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
        "\n",
        "        return self.norm2(x + y), attn\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.attn_layers = nn.ModuleList(attn_layers)\n",
        "        self.conv_layers = (\n",
        "            nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
        "        )\n",
        "        self.norm = norm_layer\n",
        "\n",
        "    def forward(self, x, attn_mask=None, tau=None, delta=None):\n",
        "        # x [B, L, D]\n",
        "        attns = []\n",
        "        if self.conv_layers is not None:\n",
        "            for i, (attn_layer, conv_layer) in enumerate(\n",
        "                zip(self.attn_layers, self.conv_layers)\n",
        "            ):\n",
        "                delta = delta if i == 0 else None\n",
        "                x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n",
        "                x = conv_layer(x)\n",
        "                attns.append(attn)\n",
        "            x, attn = self.attn_layers[-1](x, tau=tau, delta=None)\n",
        "            attns.append(attn)\n",
        "        else:\n",
        "            for attn_layer in self.attn_layers:\n",
        "                x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n",
        "                attns.append(attn)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "\n",
        "        return x, attns\n",
        "\n",
        "class FullAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        mask_flag=True,\n",
        "        factor=5,\n",
        "        scale=None,\n",
        "        attention_dropout=0.1,\n",
        "        output_attention=False,\n",
        "    ):\n",
        "        super(FullAttention, self).__init__()\n",
        "        self.scale = scale\n",
        "        self.mask_flag = mask_flag\n",
        "        self.output_attention = output_attention\n",
        "        self.dropout = nn.Dropout(attention_dropout)\n",
        "\n",
        "    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):\n",
        "        B, L, H, E = queries.shape\n",
        "        _, S, _, D = values.shape\n",
        "        scale = self.scale or 1.0 / sqrt(E)\n",
        "\n",
        "        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n",
        "\n",
        "        if self.mask_flag:\n",
        "            if attn_mask is None:\n",
        "                attn_mask = TriangularCausalMask(B, L, device=queries.device)\n",
        "\n",
        "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
        "\n",
        "        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n",
        "        V = torch.einsum(\"bhls,bshd->blhd\", A, values)\n",
        "\n",
        "        if self.output_attention:\n",
        "            return V.contiguous(), A\n",
        "        else:\n",
        "            return V.contiguous(), None\n",
        "\n",
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, attention, d_model, n_heads, d_keys=None, d_values=None):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "\n",
        "        d_keys = d_keys or (d_model // n_heads)\n",
        "        d_values = d_values or (d_model // n_heads)\n",
        "\n",
        "        self.inner_attention = attention\n",
        "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
        "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
        "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
        "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):\n",
        "        B, L, _ = queries.shape\n",
        "        _, S, _ = keys.shape\n",
        "        H = self.n_heads\n",
        "\n",
        "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
        "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
        "        values = self.value_projection(values).view(B, S, H, -1)\n",
        "\n",
        "        out, attn = self.inner_attention(\n",
        "            queries, keys, values, attn_mask, tau=tau, delta=delta\n",
        "        )\n",
        "        out = out.view(B, L, -1)\n",
        "\n",
        "        return self.out_projection(out), attn\n",
        "\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "\n",
        "        # Compute the positional encodings once in log space.\n",
        "\n",
        "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
        "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model).float()\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        pe.require_grad = False\n",
        "\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pe[:, : x.size(1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac98a5fe-731f-4d71-9b6d-bc38392c31d4",
      "metadata": {
        "id": "ac98a5fe-731f-4d71-9b6d-bc38392c31d4"
      },
      "outputs": [],
      "source": [
        "class MultimodalFinancialDataset(Dataset):\n",
        "    def __init__(self, time_series_path, text_path, window_size=5, split=\"train\", max_len=390):\n",
        "        self.window_size = window_size\n",
        "        self.split = split\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # Load time-series data\n",
        "        time_series_data = pd.read_csv(time_series_path)\n",
        "\n",
        "        def process_list(x, max_len):\n",
        "            try:\n",
        "                if isinstance(x, str):\n",
        "                    values = eval(x)\n",
        "                else:\n",
        "                    values = x\n",
        "\n",
        "                values = np.array(values, dtype=np.float32)\n",
        "\n",
        "                # Handle NaN/Inf values\n",
        "                if np.any(np.isnan(values)) or np.any(np.isinf(values)):\n",
        "                    logger.warning(f\"Found NaN/Inf values in data, replacing with zeros\")\n",
        "                    values = np.nan_to_num(values, 0)\n",
        "\n",
        "                if len(values) > max_len:\n",
        "                    return values[:max_len]\n",
        "                elif len(values) < max_len:\n",
        "                    padding = np.full(max_len - len(values), values[-1])\n",
        "                    return np.concatenate([values, padding])\n",
        "                return values\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error processing value: {x[:100]}... Error: {str(e)}\")\n",
        "                return np.zeros(max_len)\n",
        "\n",
        "        print(\"Processing time series data...\")\n",
        "        for col in ['open', 'high', 'low', 'close', 'volume']:\n",
        "            time_series_data[col] = time_series_data[col].apply(\n",
        "                lambda x: process_list(x, max_len)\n",
        "            )\n",
        "\n",
        "        print(\"Loading text data...\")\n",
        "        text_data = pd.read_csv(text_path)\n",
        "\n",
        "        print(\"Merging datasets...\")\n",
        "        self.data = pd.merge(\n",
        "            time_series_data,\n",
        "            text_data,\n",
        "            left_on='date',\n",
        "            right_on='date',\n",
        "            how='inner'\n",
        "        )\n",
        "\n",
        "        # Store dates as strings instead of timestamps\n",
        "        self.dates = pd.to_datetime(self.data['date']).dt.strftime('%Y-%m-%d').values\n",
        "\n",
        "        print(\"Processing features...\")\n",
        "        self.features = []\n",
        "        for _, row in self.data.iterrows():\n",
        "            try:\n",
        "                daily_features = np.stack([\n",
        "                    row['open'],\n",
        "                    row['high'],\n",
        "                    row['low'],\n",
        "                    row['close'],\n",
        "                    row['volume']\n",
        "                ])\n",
        "                self.features.append(daily_features)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing row for date {row['date']}: {str(e)}\")\n",
        "                daily_features = np.zeros((5, max_len))\n",
        "                self.features.append(daily_features)\n",
        "\n",
        "        self.features = np.array(self.features)\n",
        "        self.text = self.data['text'].values\n",
        "\n",
        "        print(f\"Dataset shapes:\")\n",
        "        print(f\"Features: {self.features.shape}\")\n",
        "        print(f\"Number of text samples: {len(self.text)}\")\n",
        "\n",
        "        self._compute_feature_stats()\n",
        "\n",
        "        # Generate labels - up/down based on closing price\n",
        "        self.labels = []\n",
        "        for i in range(len(self.features) - window_size + 1):\n",
        "            window_data = self.features[i:i + window_size]\n",
        "            # Compare last day's close to first day's close\n",
        "            start_price = window_data[0][3, 0]  # First day's opening price\n",
        "            end_price = window_data[-1][3, -1]  # Last day's closing price\n",
        "            # 1 if price went up, 0 if down\n",
        "            label = 1 if end_price > start_price else 0\n",
        "            self.labels.append(label)\n",
        "\n",
        "        self.labels = np.array(self.labels)\n",
        "\n",
        "    def _compute_feature_stats(self):\n",
        "        \"\"\"Compute feature statistics with validation\"\"\"\n",
        "        try:\n",
        "            self.feature_stats = {\n",
        "                'mean': np.nanmean(self.features[:, :4, :], axis=(0, 2)),\n",
        "                'std': np.nanstd(self.features[:, :4, :], axis=(0, 2)),\n",
        "                'volume_mean': np.nanmean(self.features[:, 4, :]),\n",
        "                'volume_std': np.nanstd(self.features[:, 4, :])\n",
        "            }\n",
        "\n",
        "            # Handle zero standard deviations\n",
        "            self.feature_stats['std'] = np.where(\n",
        "                self.feature_stats['std'] == 0,\n",
        "                1e-6,  # Small constant instead of 1\n",
        "                self.feature_stats['std']\n",
        "            )\n",
        "\n",
        "            if self.feature_stats['volume_std'] == 0:\n",
        "                self.feature_stats['volume_std'] = 1e-6\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error computing feature statistics: {str(e)}\")\n",
        "\n",
        "    def normalize_features(self):\n",
        "        for i in range(4):\n",
        "            self.features[:, i, :] = (\n",
        "                (self.features[:, i, :] - self.feature_stats['mean'][i]) /\n",
        "                self.feature_stats['std'][i]\n",
        "            )\n",
        "\n",
        "        self.features[:, 4, :] = (\n",
        "            (np.log1p(self.features[:, 4, :]) - np.log1p(self.feature_stats['volume_mean'])) /\n",
        "            (self.feature_stats['volume_std'] + 1e-8)\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features) - self.window_size + 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # Ensure the idx is an integer\n",
        "        if isinstance(idx, slice):\n",
        "            raise TypeError(\"Slicing the dataset directly is not supported. Use DataLoader or manual splitting.\")\n",
        "        if idx + self.window_size > len(self.features):\n",
        "            raise IndexError(\"Index out of range for the dataset.\")\n",
        "        # Get window of data\n",
        "        x = self.features[idx:idx + self.window_size]\n",
        "        text_window = self.text[idx:idx + self.window_size]\n",
        "        dates_window = self.dates[idx:idx + self.window_size]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Calculate daily summaries\n",
        "        daily_summaries = []\n",
        "        for day_idx in range(self.window_size):\n",
        "            daily_data = x[day_idx]\n",
        "\n",
        "            valid_idx = np.where(np.diff(daily_data[3]) != 0)[0]\n",
        "            seq_len = valid_idx[-1] + 1 if len(valid_idx) > 0 else daily_data.shape[1]\n",
        "\n",
        "            summary = {\n",
        "                'date': dates_window[day_idx],\n",
        "                'day_open': daily_data[0, 0],\n",
        "                'day_close': daily_data[3, seq_len-1],\n",
        "                'day_high': np.max(daily_data[1, :seq_len]),\n",
        "                'day_low': np.min(daily_data[2, :seq_len]),\n",
        "                'day_volume': np.sum(daily_data[4, :seq_len]),\n",
        "                'volatility': np.std(daily_data[3, :seq_len]),\n",
        "                'text': text_window[day_idx]\n",
        "            }\n",
        "\n",
        "            prompt = (\n",
        "                f\"Date: {summary['date']} | \"\n",
        "                f\"Open: {summary['day_open']:.2f} | \"\n",
        "                f\"Close: {summary['day_close']:.2f} | \"\n",
        "                f\"High: {summary['day_high']:.2f} | \"\n",
        "                f\"Low: {summary['day_low']:.2f} | \"\n",
        "                f\"Volume: {summary['day_volume']:,.0f} | \"\n",
        "                f\"Volatility: {summary['volatility']:.4f} | \"\n",
        "                f\"Context: {summary['text']}\"\n",
        "            )\n",
        "            daily_summaries.append(prompt)\n",
        "\n",
        "        return {\n",
        "            \"x_enc\": torch.tensor(x, dtype=torch.float32),\n",
        "            \"text\": daily_summaries,\n",
        "            \"dates\": dates_window.tolist(),  # Convert numpy array to list\n",
        "            \"label\": torch.tensor(label, dtype=torch.long)  # Add label to return dict\n",
        "        }\n",
        "\n",
        "# Custom collate function to handle the batch creation\n",
        "def custom_collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function to properly batch the data.\n",
        "    \"\"\"\n",
        "    x_enc = torch.stack([item['x_enc'] for item in batch])\n",
        "    text = [item['text'] for item in batch]\n",
        "    dates = [item['dates'] for item in batch]\n",
        "    labels = torch.stack([item['label'] for item in batch])\n",
        "\n",
        "    return {\n",
        "        'x_enc': x_enc,\n",
        "        'text': text,\n",
        "        'dates': dates,\n",
        "        'labels': labels\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0442cUppotMT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0442cUppotMT",
        "outputId": "fe61f776-53e8-46ea-839c-c1b6d784fb46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing time series data...\n",
            "Loading text data...\n",
            "Merging datasets...\n",
            "Processing features...\n",
            "Dataset shapes:\n",
            "Features: (334, 5, 390)\n",
            "Number of text samples: 334\n",
            "\n",
            "Processed Dataset Sample:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Sample 1:\n",
            "Dates: ['2021-01-04', '2021-01-05', '2021-01-06', '2021-01-07', '2021-01-08']\n",
            "\n",
            "Time Series Data Summary:\n",
            "\n",
            "Day 1 (2021-01-04):\n",
            "Open:  Mean=-1.067, Min=-1.097, Max=-1.041\n",
            "High:  Mean=-1.066, Min=-1.092, Max=-1.037\n",
            "Low:   Mean=-1.067, Min=-1.098, Max=-1.047\n",
            "Close: Mean=-1.067, Min=-1.097, Max=-1.042\n",
            "Volume: Mean=-0.000, Min=-0.000, Max=0.000\n",
            "\n",
            "Day 2 (2021-01-05):\n",
            "Open:  Mean=-0.971, Min=-1.020, Max=-0.943\n",
            "High:  Mean=-0.971, Min=-1.021, Max=-0.942\n",
            "Low:   Mean=-0.971, Min=-1.023, Max=-0.945\n",
            "Close: Mean=-0.971, Min=-1.020, Max=-0.944\n",
            "Volume: Mean=-0.000, Min=-0.000, Max=0.000\n",
            "\n",
            "Day 3 (2021-01-06):\n",
            "Open:  Mean=-1.157, Min=-1.225, Max=-1.047\n",
            "High:  Mean=-1.155, Min=-1.216, Max=-1.045\n",
            "Low:   Mean=-1.158, Min=-1.226, Max=-1.049\n",
            "Close: Mean=-1.157, Min=-1.225, Max=-1.047\n",
            "Volume: Mean=-0.000, Min=-0.000, Max=0.000\n",
            "\n",
            "Day 4 (2021-01-07):\n",
            "Open:  Mean=-0.975, Min=-1.007, Max=-0.914\n",
            "High:  Mean=-0.975, Min=-1.007, Max=-0.914\n",
            "Low:   Mean=-0.975, Min=-1.008, Max=-0.915\n",
            "Close: Mean=-0.975, Min=-1.007, Max=-0.913\n",
            "Volume: Mean=-0.000, Min=-0.000, Max=0.000\n",
            "\n",
            "Day 5 (2021-01-08):\n",
            "Open:  Mean=-0.925, Min=-0.999, Max=-0.899\n",
            "High:  Mean=-0.925, Min=-0.995, Max=-0.899\n",
            "Low:   Mean=-0.926, Min=-1.007, Max=-0.899\n",
            "Close: Mean=-0.925, Min=-1.000, Max=-0.898\n",
            "Volume: Mean=-0.000, Min=-0.000, Max=0.000\n",
            "\n",
            "Text Prompts:\n",
            "Day 1: Date: 2021-01-04 | Open: -1.05 | Close: -1.09 | High: -1.04 | Low: -1.10 | Volume: -0 | Volatility: 0.0109 | Context: rt at user loup ventures gene munster believes that digital acceleration the iphon...\n",
            "Day 2: Date: 2021-01-05 | Open: -0.98 | Close: -1.01 | High: -0.94 | Low: -1.02 | Volume: -0 | Volatility: 0.0152 | Context: $ aapl has reacted higher from the extreme area and members are already risk free ...\n",
            "Day 3: Date: 2021-01-06 | Open: -1.15 | Close: -1.07 | High: -1.05 | Low: -1.23 | Volume: -0 | Volatility: 0.0413 | Context: $ nio up 1 ah $ tsla $ spy $ spx $ qqq $ ndx $ amzn $ aapl $ fb $ goog $ msft $ sh...\n",
            "Day 4: Date: 2021-01-07 | Open: -0.93 | Close: -1.00 | High: -0.91 | Low: -1.01 | Volume: -0 | Volatility: 0.0205 | Context: $ aapl apple estimates continue to inch higher driven by strong iphone 12 demand u...\n",
            "Day 5: Date: 2021-01-08 | Open: -0.92 | Close: -0.92 | High: -0.90 | Low: -1.01 | Volume: -0 | Volatility: 0.0219 | Context: hyundai says in early talks with apple after electric vehicle tie up report $ aapl...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Sample 2:\n",
            "Dates: ['2021-01-05', '2021-01-06', '2021-01-07', '2021-01-08', '2021-01-11']\n",
            "\n",
            "Time Series Data Summary:\n",
            "\n",
            "Day 1 (2021-01-05):\n",
            "Open:  Mean=-0.971, Min=-1.020, Max=-0.943\n",
            "High:  Mean=-0.971, Min=-1.021, Max=-0.942\n",
            "Low:   Mean=-0.971, Min=-1.023, Max=-0.945\n",
            "Close: Mean=-0.971, Min=-1.020, Max=-0.944\n",
            "Volume: Mean=-0.000, Min=-0.000, Max=0.000\n",
            "\n",
            "Day 2 (2021-01-06):\n",
            "Open:  Mean=-1.157, Min=-1.225, Max=-1.047\n",
            "High:  Mean=-1.155, Min=-1.216, Max=-1.045\n",
            "Low:   Mean=-1.158, Min=-1.226, Max=-1.049\n",
            "Close: Mean=-1.157, Min=-1.225, Max=-1.047\n",
            "Volume: Mean=-0.000, Min=-0.000, Max=0.000\n",
            "\n",
            "Day 3 (2021-01-07):\n",
            "Open:  Mean=-0.975, Min=-1.007, Max=-0.914\n",
            "High:  Mean=-0.975, Min=-1.007, Max=-0.914\n",
            "Low:   Mean=-0.975, Min=-1.008, Max=-0.915\n",
            "Close: Mean=-0.975, Min=-1.007, Max=-0.913\n",
            "Volume: Mean=-0.000, Min=-0.000, Max=0.000\n",
            "\n",
            "Day 4 (2021-01-08):\n",
            "Open:  Mean=-0.925, Min=-0.999, Max=-0.899\n",
            "High:  Mean=-0.925, Min=-0.995, Max=-0.899\n",
            "Low:   Mean=-0.926, Min=-1.007, Max=-0.899\n",
            "Close: Mean=-0.925, Min=-1.000, Max=-0.898\n",
            "Volume: Mean=-0.000, Min=-0.000, Max=0.000\n",
            "\n",
            "Day 5 (2021-01-11):\n",
            "Open:  Mean=-1.071, Min=-1.092, Max=-1.027\n",
            "High:  Mean=-1.071, Min=-1.091, Max=-1.027\n",
            "Low:   Mean=-1.071, Min=-1.093, Max=-1.028\n",
            "Close: Mean=-1.072, Min=-1.092, Max=-1.027\n",
            "Volume: Mean=-0.000, Min=-0.000, Max=0.000\n",
            "\n",
            "Text Prompts:\n",
            "Day 1: Date: 2021-01-05 | Open: -0.98 | Close: -1.01 | High: -0.94 | Low: -1.02 | Volume: -0 | Volatility: 0.0152 | Context: $ aapl has reacted higher from the extreme area and members are already risk free ...\n",
            "Day 2: Date: 2021-01-06 | Open: -1.15 | Close: -1.07 | High: -1.05 | Low: -1.23 | Volume: -0 | Volatility: 0.0413 | Context: $ nio up 1 ah $ tsla $ spy $ spx $ qqq $ ndx $ amzn $ aapl $ fb $ goog $ msft $ sh...\n",
            "Day 3: Date: 2021-01-07 | Open: -0.93 | Close: -1.00 | High: -0.91 | Low: -1.01 | Volume: -0 | Volatility: 0.0205 | Context: $ aapl apple estimates continue to inch higher driven by strong iphone 12 demand u...\n",
            "Day 4: Date: 2021-01-08 | Open: -0.92 | Close: -0.92 | High: -0.90 | Low: -1.01 | Volume: -0 | Volatility: 0.0219 | Context: hyundai says in early talks with apple after electric vehicle tie up report $ aapl...\n",
            "Day 5: Date: 2021-01-11 | Open: -1.07 | Close: -1.03 | High: -1.03 | Low: -1.09 | Volume: -0 | Volatility: 0.0101 | Context: rt at user marketwatch big tech companies close lower amid censorship moves $ twtr...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Sample 3:\n",
            "Dates: ['2021-01-06', '2021-01-07', '2021-01-08', '2021-01-11', '2021-01-12']\n",
            "\n",
            "Time Series Data Summary:\n",
            "\n",
            "Day 1 (2021-01-06):\n",
            "Open:  Mean=-1.157, Min=-1.225, Max=-1.047\n",
            "High:  Mean=-1.155, Min=-1.216, Max=-1.045\n",
            "Low:   Mean=-1.158, Min=-1.226, Max=-1.049\n",
            "Close: Mean=-1.157, Min=-1.225, Max=-1.047\n",
            "Volume: Mean=-0.000, Min=-0.000, Max=0.000\n",
            "\n",
            "Day 2 (2021-01-07):\n",
            "Open:  Mean=-0.975, Min=-1.007, Max=-0.914\n",
            "High:  Mean=-0.975, Min=-1.007, Max=-0.914\n",
            "Low:   Mean=-0.975, Min=-1.008, Max=-0.915\n",
            "Close: Mean=-0.975, Min=-1.007, Max=-0.913\n",
            "Volume: Mean=-0.000, Min=-0.000, Max=0.000\n",
            "\n",
            "Day 3 (2021-01-08):\n",
            "Open:  Mean=-0.925, Min=-0.999, Max=-0.899\n",
            "High:  Mean=-0.925, Min=-0.995, Max=-0.899\n",
            "Low:   Mean=-0.926, Min=-1.007, Max=-0.899\n",
            "Close: Mean=-0.925, Min=-1.000, Max=-0.898\n",
            "Volume: Mean=-0.000, Min=-0.000, Max=0.000\n",
            "\n",
            "Day 4 (2021-01-11):\n",
            "Open:  Mean=-1.071, Min=-1.092, Max=-1.027\n",
            "High:  Mean=-1.071, Min=-1.091, Max=-1.027\n",
            "Low:   Mean=-1.071, Min=-1.093, Max=-1.028\n",
            "Close: Mean=-1.072, Min=-1.092, Max=-1.027\n",
            "Volume: Mean=-0.000, Min=-0.000, Max=0.000\n",
            "\n",
            "Day 5 (2021-01-12):\n",
            "Open:  Mean=-1.092, Min=-1.140, Max=-1.064\n",
            "High:  Mean=-1.092, Min=-1.137, Max=-1.066\n",
            "Low:   Mean=-1.092, Min=-1.140, Max=-1.064\n",
            "Close: Mean=-1.092, Min=-1.140, Max=-1.064\n",
            "Volume: Mean=-0.000, Min=-0.000, Max=0.000\n",
            "\n",
            "Text Prompts:\n",
            "Day 1: Date: 2021-01-06 | Open: -1.15 | Close: -1.07 | High: -1.05 | Low: -1.23 | Volume: -0 | Volatility: 0.0413 | Context: $ nio up 1 ah $ tsla $ spy $ spx $ qqq $ ndx $ amzn $ aapl $ fb $ goog $ msft $ sh...\n",
            "Day 2: Date: 2021-01-07 | Open: -0.93 | Close: -1.00 | High: -0.91 | Low: -1.01 | Volume: -0 | Volatility: 0.0205 | Context: $ aapl apple estimates continue to inch higher driven by strong iphone 12 demand u...\n",
            "Day 3: Date: 2021-01-08 | Open: -0.92 | Close: -0.92 | High: -0.90 | Low: -1.01 | Volume: -0 | Volatility: 0.0219 | Context: hyundai says in early talks with apple after electric vehicle tie up report $ aapl...\n",
            "Day 4: Date: 2021-01-11 | Open: -1.07 | Close: -1.03 | High: -1.03 | Low: -1.09 | Volume: -0 | Volatility: 0.0101 | Context: rt at user marketwatch big tech companies close lower amid censorship moves $ twtr...\n",
            "Day 5: Date: 2021-01-12 | Open: -1.07 | Close: -1.14 | High: -1.07 | Low: -1.14 | Volume: -0 | Volatility: 0.0164 | Context: rt at user as i posted last week $ aapl looked weak bulls need to go back above $ ...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Sample 4:\n",
            "Dates: ['2021-01-07', '2021-01-08', '2021-01-11', '2021-01-12', '2021-01-13']\n",
            "\n",
            "Time Series Data Summary:\n",
            "\n",
            "Day 1 (2021-01-07):\n",
            "Open:  Mean=-0.975, Min=-1.007, Max=-0.914\n",
            "High:  Mean=-0.975, Min=-1.007, Max=-0.914\n",
            "Low:   Mean=-0.975, Min=-1.008, Max=-0.915\n",
            "Close: Mean=-0.975, Min=-1.007, Max=-0.913\n",
            "Volume: Mean=-0.000, Min=-0.000, Max=0.000\n",
            "\n",
            "Day 2 (2021-01-08):\n",
            "Open:  Mean=-0.925, Min=-0.999, Max=-0.899\n",
            "High:  Mean=-0.925, Min=-0.995, Max=-0.899\n",
            "Low:   Mean=-0.926, Min=-1.007, Max=-0.899\n",
            "Close: Mean=-0.925, Min=-1.000, Max=-0.898\n",
            "Volume: Mean=-0.000, Min=-0.000, Max=0.000\n",
            "\n",
            "Day 3 (2021-01-11):\n",
            "Open:  Mean=-1.071, Min=-1.092, Max=-1.027\n",
            "High:  Mean=-1.071, Min=-1.091, Max=-1.027\n",
            "Low:   Mean=-1.071, Min=-1.093, Max=-1.028\n",
            "Close: Mean=-1.072, Min=-1.092, Max=-1.027\n",
            "Volume: Mean=-0.000, Min=-0.000, Max=0.000\n",
            "\n",
            "Day 4 (2021-01-12):\n",
            "Open:  Mean=-1.092, Min=-1.140, Max=-1.064\n",
            "High:  Mean=-1.092, Min=-1.137, Max=-1.066\n",
            "Low:   Mean=-1.092, Min=-1.140, Max=-1.064\n",
            "Close: Mean=-1.092, Min=-1.140, Max=-1.064\n",
            "Volume: Mean=-0.000, Min=-0.000, Max=0.000\n",
            "\n",
            "Day 5 (2021-01-13):\n",
            "Open:  Mean=-0.968, Min=-0.986, Max=-0.951\n",
            "High:  Mean=-0.969, Min=-0.983, Max=-0.951\n",
            "Low:   Mean=-0.968, Min=-0.986, Max=-0.950\n",
            "Close: Mean=-0.968, Min=-0.985, Max=-0.949\n",
            "Volume: Mean=-0.000, Min=-0.000, Max=0.000\n",
            "\n",
            "Text Prompts:\n",
            "Day 1: Date: 2021-01-07 | Open: -0.93 | Close: -1.00 | High: -0.91 | Low: -1.01 | Volume: -0 | Volatility: 0.0205 | Context: $ aapl apple estimates continue to inch higher driven by strong iphone 12 demand u...\n",
            "Day 2: Date: 2021-01-08 | Open: -0.92 | Close: -0.92 | High: -0.90 | Low: -1.01 | Volume: -0 | Volatility: 0.0219 | Context: hyundai says in early talks with apple after electric vehicle tie up report $ aapl...\n",
            "Day 3: Date: 2021-01-11 | Open: -1.07 | Close: -1.03 | High: -1.03 | Low: -1.09 | Volume: -0 | Volatility: 0.0101 | Context: rt at user marketwatch big tech companies close lower amid censorship moves $ twtr...\n",
            "Day 4: Date: 2021-01-12 | Open: -1.07 | Close: -1.14 | High: -1.07 | Low: -1.14 | Volume: -0 | Volatility: 0.0164 | Context: rt at user as i posted last week $ aapl looked weak bulls need to go back above $ ...\n",
            "Day 5: Date: 2021-01-13 | Open: -0.95 | Close: -0.96 | High: -0.95 | Low: -0.99 | Volume: -0 | Volatility: 0.0070 | Context: $ aapl, rt at user $ viac $ cmcsa $ dis $ nflx $ aapl $ t $ sne $ roku $ amzn $ di...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Sample 5:\n",
            "Dates: ['2021-01-08', '2021-01-11', '2021-01-12', '2021-01-13', '2021-01-14']\n",
            "\n",
            "Time Series Data Summary:\n",
            "\n",
            "Day 1 (2021-01-08):\n",
            "Open:  Mean=-0.925, Min=-0.999, Max=-0.899\n",
            "High:  Mean=-0.925, Min=-0.995, Max=-0.899\n",
            "Low:   Mean=-0.926, Min=-1.007, Max=-0.899\n",
            "Close: Mean=-0.925, Min=-1.000, Max=-0.898\n",
            "Volume: Mean=-0.000, Min=-0.000, Max=0.000\n",
            "\n",
            "Day 2 (2021-01-11):\n",
            "Open:  Mean=-1.071, Min=-1.092, Max=-1.027\n",
            "High:  Mean=-1.071, Min=-1.091, Max=-1.027\n",
            "Low:   Mean=-1.071, Min=-1.093, Max=-1.028\n",
            "Close: Mean=-1.072, Min=-1.092, Max=-1.027\n",
            "Volume: Mean=-0.000, Min=-0.000, Max=0.000\n",
            "\n",
            "Day 3 (2021-01-12):\n",
            "Open:  Mean=-1.092, Min=-1.140, Max=-1.064\n",
            "High:  Mean=-1.092, Min=-1.137, Max=-1.066\n",
            "Low:   Mean=-1.092, Min=-1.140, Max=-1.064\n",
            "Close: Mean=-1.092, Min=-1.140, Max=-1.064\n",
            "Volume: Mean=-0.000, Min=-0.000, Max=0.000\n",
            "\n",
            "Day 4 (2021-01-13):\n",
            "Open:  Mean=-0.968, Min=-0.986, Max=-0.951\n",
            "High:  Mean=-0.969, Min=-0.983, Max=-0.951\n",
            "Low:   Mean=-0.968, Min=-0.986, Max=-0.950\n",
            "Close: Mean=-0.968, Min=-0.985, Max=-0.949\n",
            "Volume: Mean=-0.000, Min=-0.000, Max=0.000\n",
            "\n",
            "Day 5 (2021-01-14):\n",
            "Open:  Mean=-1.061, Min=-1.087, Max=-1.025\n",
            "High:  Mean=-1.061, Min=-1.083, Max=-1.019\n",
            "Low:   Mean=-1.061, Min=-1.089, Max=-1.027\n",
            "Close: Mean=-1.061, Min=-1.087, Max=-1.024\n",
            "Volume: Mean=-0.000, Min=-0.000, Max=0.000\n",
            "\n",
            "Text Prompts:\n",
            "Day 1: Date: 2021-01-08 | Open: -0.92 | Close: -0.92 | High: -0.90 | Low: -1.01 | Volume: -0 | Volatility: 0.0219 | Context: hyundai says in early talks with apple after electric vehicle tie up report $ aapl...\n",
            "Day 2: Date: 2021-01-11 | Open: -1.07 | Close: -1.03 | High: -1.03 | Low: -1.09 | Volume: -0 | Volatility: 0.0101 | Context: rt at user marketwatch big tech companies close lower amid censorship moves $ twtr...\n",
            "Day 3: Date: 2021-01-12 | Open: -1.07 | Close: -1.14 | High: -1.07 | Low: -1.14 | Volume: -0 | Volatility: 0.0164 | Context: rt at user as i posted last week $ aapl looked weak bulls need to go back above $ ...\n",
            "Day 4: Date: 2021-01-13 | Open: -0.95 | Close: -0.96 | High: -0.95 | Low: -0.99 | Volume: -0 | Volatility: 0.0070 | Context: $ aapl, rt at user $ viac $ cmcsa $ dis $ nflx $ aapl $ t $ sne $ roku $ amzn $ di...\n",
            "Day 5: Date: 2021-01-14 | Open: -1.07 | Close: -1.03 | High: -1.02 | Low: -1.09 | Volume: -0 | Volatility: 0.0129 | Context: $ aapl is 1 stock every investor should own in their portfolio . i haven t sold 1 ...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Batch shapes:\n",
            "x_enc: torch.Size([32, 5, 5, 390])\n",
            "Number of text samples: 32\n",
            "Number of date samples: 32\n"
          ]
        }
      ],
      "source": [
        "# Add this after creating the dataset but before the dataloader\n",
        "def display_processed_dataset(dataset, num_samples=5):\n",
        "    print(\"\\nProcessed Dataset Sample:\")\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    for i in range(min(num_samples, len(dataset))):\n",
        "        sample = dataset[i]\n",
        "\n",
        "        print(f\"\\nSample {i+1}:\")\n",
        "        print(\"Dates:\", sample['dates'])\n",
        "\n",
        "        # Print time series data summary\n",
        "        ts_data = sample['x_enc']  # Shape: [window_size, features, minutes]\n",
        "        print(\"\\nTime Series Data Summary:\")\n",
        "        for day in range(ts_data.size(0)):\n",
        "            print(f\"\\nDay {day+1} ({sample['dates'][day]}):\")\n",
        "            print(f\"Open:  Mean={ts_data[day,0,:].mean():.3f}, Min={ts_data[day,0,:].min():.3f}, Max={ts_data[day,0,:].max():.3f}\")\n",
        "            print(f\"High:  Mean={ts_data[day,1,:].mean():.3f}, Min={ts_data[day,1,:].min():.3f}, Max={ts_data[day,1,:].max():.3f}\")\n",
        "            print(f\"Low:   Mean={ts_data[day,2,:].mean():.3f}, Min={ts_data[day,2,:].min():.3f}, Max={ts_data[day,2,:].max():.3f}\")\n",
        "            print(f\"Close: Mean={ts_data[day,3,:].mean():.3f}, Min={ts_data[day,3,:].min():.3f}, Max={ts_data[day,3,:].max():.3f}\")\n",
        "            print(f\"Volume: Mean={ts_data[day,4,:].mean():.3f}, Min={ts_data[day,4,:].min():.3f}, Max={ts_data[day,4,:].max():.3f}\")\n",
        "\n",
        "        print(\"\\nText Prompts:\")\n",
        "        for j, prompt in enumerate(sample['text']):\n",
        "            print(f\"Day {j+1}: {prompt[:200]}...\")\n",
        "\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "# Create dataset\n",
        "dataset = MultimodalFinancialDataset(\n",
        "    time_series_path='AAPL_train_data_aggregated.csv',\n",
        "    text_path='AAPL_tweets_train.csv',\n",
        "    window_size=5,\n",
        "    max_len=390\n",
        ")\n",
        "\n",
        "# Normalize features\n",
        "dataset.normalize_features()\n",
        "\n",
        "# Display processed samples\n",
        "display_processed_dataset(dataset, num_samples=5)\n",
        "\n",
        "# Create data loader with custom collate function\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    collate_fn=custom_collate_fn  # Add this line\n",
        ")\n",
        "\n",
        "# Test the dataloader\n",
        "batch = next(iter(dataloader))\n",
        "print(\"Batch shapes:\")\n",
        "print(f\"x_enc: {batch['x_enc'].shape}\")\n",
        "print(f\"Number of text samples: {len(batch['text'])}\")\n",
        "print(f\"Number of date samples: {len(batch['dates'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hVoHuW-cp4ZJ",
      "metadata": {
        "id": "hVoHuW-cp4ZJ"
      },
      "source": [
        "```\n",
        "Batch shapes: x_enc: torch.Size([32, 5, 5, 390])\n",
        "               |    |  |  |\n",
        "               |    |  |  └── Number of minutes in each trading day (390 = 6.5 hours × 60 minutes)\n",
        "               |    |  └── Number of features (OHLCV: Open, High, Low, Close, Volume)\n",
        "               |    └── Window size (5 days of historical data)\n",
        "               └── Batch size (32 samples per batch)\n",
        "\n",
        "Number of text samples: 32\n",
        "Number of date samples: 32\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fd0c342-e6ea-4892-9530-2bdc97076b20",
      "metadata": {
        "id": "1fd0c342-e6ea-4892-9530-2bdc97076b20"
      },
      "source": [
        "Patch-Based Embedding for Time-Series : https://github.com/flixpar/med-ts-llm/blob/main/models/PatchTST.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e54840e-5742-49d9-bf58-5eab1752a062",
      "metadata": {
        "id": "7e54840e-5742-49d9-bf58-5eab1752a062"
      },
      "outputs": [],
      "source": [
        "# Modified PatchEmbedding class\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, patch_len, stride, padding, dropout):\n",
        "        super().__init__()\n",
        "        self.padding_patch_layer = nn.ReplicationPad1d((0, padding))\n",
        "        # Modified to handle combined features in each patch\n",
        "        self.value_embedding = nn.Linear(patch_len * 5, d_model, bias=False)  # 5 features combined\n",
        "        self.position_embedding = PositionalEmbedding(d_model, max_len=1024)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.patch_len = patch_len\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape [batch_size, window_size, n_features, n_minutes]\n",
        "               where batch_size=32, window_size=5, n_features=5, n_minutes=390\n",
        "        \"\"\"\n",
        "        batch_size, window_size, n_features, n_minutes = x.shape\n",
        "\n",
        "        # Reshape to handle windows independently\n",
        "        x = x.reshape(-1, n_features, n_minutes)  # [batch_size * window_size, n_features, n_minutes]\n",
        "\n",
        "        # Apply padding\n",
        "        x = self.padding_patch_layer(x)  # [batch_size * window_size, n_features, n_minutes + padding]\n",
        "\n",
        "        # Create patches - unfold each feature sequence\n",
        "        # [batch_size * window_size, n_features, num_patches, patch_len]\n",
        "        x = x.unfold(dimension=-1, size=self.patch_len, step=self.stride)\n",
        "        num_patches = x.size(2)\n",
        "\n",
        "        # Reshape to combine features within each patch\n",
        "        x = x.permute(0, 2, 1, 3)  # [batch_size * window_size, num_patches, n_features, patch_len]\n",
        "        x = x.reshape(batch_size * window_size, num_patches, -1)  # [batch_size * window_size, num_patches, n_features * patch_len]\n",
        "\n",
        "        # Apply embeddings\n",
        "        x = self.value_embedding(x)  # [batch_size * window_size, num_patches, d_model]\n",
        "        x = x + self.position_embedding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Reshape back to include window dimension\n",
        "        x = x.reshape(batch_size, window_size, num_patches, -1)\n",
        "\n",
        "        return x, n_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LrzyivmwtH4u",
      "metadata": {
        "id": "LrzyivmwtH4u"
      },
      "outputs": [],
      "source": [
        "class PatchTSTWithBERT(nn.Module):\n",
        "    def __init__(self, config, dataset, bert_model=\"bert-base-uncased\"):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.model_config = config['models']['patchtst']\n",
        "\n",
        "        # Validate input dimensions\n",
        "        if not hasattr(dataset, 'features'):\n",
        "            raise ValueError(\"Dataset must have 'features' attribute\")\n",
        "\n",
        "        self.enc_in = dataset.features.shape[1]  # Number of input features (5)\n",
        "        self.num_class = 2 # Binary segmentation\n",
        "        self.max_seq_len = dataset.features.shape[2] # Number of minutes (390)\n",
        "\n",
        "        print(f\"Input features shape: {dataset.features.shape}\")\n",
        "        print(f\"Patch length: {self.model_config['patching']['patch_len']}\")\n",
        "        print(f\"Stride: {self.model_config['patching']['stride']}\")\n",
        "\n",
        "        self.n_patches = self._calculate_n_patches()\n",
        "        self.projection_dim = self._calculate_projection_dim()\n",
        "\n",
        "        print(f\"Final projection dim: {self.projection_dim}\")\n",
        "\n",
        "        # Create projection layer\n",
        "        self.projection = nn.Linear(self.projection_dim, self.num_class)\n",
        "\n",
        "        # # Calculate patches based on minute-level sequence length\n",
        "        # self.n_patches = (self.max_seq_len - self.model_config['patching']['patch_len']) // self.model_config['patching']['stride'] + 1\n",
        "        # print(f\"Number of patches per sequence: {self.n_patches}\")\n",
        "\n",
        "        # Patch-based encoding\n",
        "        self.patch_embedding = PatchEmbedding(\n",
        "            d_model=self.model_config['d_model'],\n",
        "            patch_len=self.model_config['patching']['patch_len'],\n",
        "            stride=self.model_config['patching']['stride'],\n",
        "            padding=self.model_config['patching']['stride'],\n",
        "            dropout=config['training']['dropout'],\n",
        "        )\n",
        "\n",
        "        # Transformer encoder\n",
        "        self.encoder = Encoder(\n",
        "            [\n",
        "                EncoderLayer(\n",
        "                    AttentionLayer(\n",
        "                        FullAttention(False, factor=3, attention_dropout=config['training']['dropout']),\n",
        "                        self.model_config['d_model'],\n",
        "                        self.model_config['n_heads'],\n",
        "                    ),\n",
        "                    self.model_config['d_model'],\n",
        "                    self.model_config['d_ff'],\n",
        "                    dropout=config['training']['dropout'],\n",
        "                    activation=\"gelu\",\n",
        "                )\n",
        "                for _ in range(self.model_config['e_layers'])\n",
        "            ],\n",
        "            norm_layer=nn.LayerNorm(self.model_config['d_model']),\n",
        "        )\n",
        "\n",
        "        # BERT integration\n",
        "        self.bert_tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
        "        self.bert = BertModel.from_pretrained(bert_model)\n",
        "\n",
        "        if config.get('freeze_bert', True):\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # Projections\n",
        "        self.bert_projection = nn.Linear(768, self.model_config['d_model'])\n",
        "\n",
        "        # Calculate actual flattened dimension based on shapes\n",
        "        # Each window produces (n_patches * d_model) features\n",
        "        # Total features after flattening all windows\n",
        "        self.features_per_window = (self.n_patches * self.model_config['d_model'])\n",
        "        self.total_features = self.features_per_window * config['history_len']\n",
        "        # print(f\"Features per window: {self.features_per_window}\")\n",
        "        # print(f\"Total features after flattening: {self.total_features}\")\n",
        "\n",
        "        # Projection to output classes\n",
        "        # self.projection = nn.Linear(16640, self.num_class)  # Using actual shape from tensor\n",
        "\n",
        "    def _calculate_n_patches(self):\n",
        "        \"\"\"Calculate number of patches dynamically\"\"\"\n",
        "        return (self.max_seq_len - self.model_config['patching']['patch_len']) // self.model_config['patching']['stride'] + 1\n",
        "\n",
        "    def _calculate_projection_dim(self):\n",
        "        \"\"\"Calculate projection dimension dynamically\"\"\"\n",
        "        # We know from the debug output that we get 26 patches\n",
        "        n_patches = 26  # Fixed value based on actual output\n",
        "        features_per_window = n_patches * self.model_config['d_model']\n",
        "        total_dim = features_per_window * self.config['history_len']\n",
        "        # This will be 26 * 128 * 5 = 16640, matching our tensor shape\n",
        "        return total_dim\n",
        "\n",
        "    def forward(self, x, prompts=None):\n",
        "        batch_size, window_size, n_features, n_minutes = x.shape\n",
        "        print(f\"Input shape: {x.shape}\")  # [batch_size, window_size, n_features, n_minutes]\n",
        "\n",
        "        # Apply patch embedding\n",
        "        x_patched, _ = self.patch_embedding(x)\n",
        "        print(f\"After patch embedding: {x_patched.shape}\")  # [batch_size, window_size, num_patches, d_model]\n",
        "\n",
        "        # Process text if provided\n",
        "        if prompts is not None:\n",
        "            text_embeds = self.encode_prompts(prompts)\n",
        "            text_embeds = self.align_prompt_embeddings(\n",
        "                text_embeds, batch_size, window_size, x_patched\n",
        "            )\n",
        "            x_patched = x_patched + text_embeds\n",
        "            print(f\"After text fusion: {x_patched.shape}\")\n",
        "\n",
        "        # Reshape for encoder\n",
        "        x_patched = x_patched.reshape(-1, x_patched.size(2), x_patched.size(3))\n",
        "        print(f\"Before encoder: {x_patched.shape}\")\n",
        "\n",
        "        # Pass through encoder\n",
        "        x_encoded, _ = self.encoder(x_patched)\n",
        "        print(f\"After encoder: {x_encoded.shape}\")\n",
        "\n",
        "        # Reshape back to include window dimension\n",
        "        x_encoded = x_encoded.reshape(batch_size, window_size, -1)\n",
        "        print(f\"After reshaping: {x_encoded.shape}\")\n",
        "\n",
        "        # Final flatten\n",
        "        x_encoded = x_encoded.reshape(batch_size, -1)\n",
        "        print(f\"Final shape before projection: {x_encoded.shape}\")\n",
        "        print(f\"Projection weight shape: {self.projection.weight.shape}\")\n",
        "\n",
        "        return self.projection(x_encoded)\n",
        "\n",
        "    def encode_prompts(self, prompts):\n",
        "        \"\"\"Encodes text prompts using BERT.\"\"\"\n",
        "        batch_size = len(prompts)\n",
        "        window_size = len(prompts[0])\n",
        "        all_embeddings = []\n",
        "\n",
        "        for batch_idx in range(batch_size):\n",
        "            window_embeddings = []\n",
        "            for window_idx in range(window_size):\n",
        "                encoded = self.bert_tokenizer(\n",
        "                    prompts[batch_idx][window_idx],\n",
        "                    padding='max_length',\n",
        "                    truncation=True,\n",
        "                    max_length=128,\n",
        "                    return_tensors=\"pt\"\n",
        "                ).to(next(self.bert.parameters()).device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    outputs = self.bert(**encoded)\n",
        "                    embedding = outputs.last_hidden_state[:, 0, :]  # Use [CLS] token\n",
        "                    window_embeddings.append(embedding)\n",
        "\n",
        "            batch_embeddings = torch.cat(window_embeddings, dim=0)\n",
        "            all_embeddings.append(batch_embeddings)\n",
        "\n",
        "        all_embeddings = torch.stack(all_embeddings)\n",
        "        return self.bert_projection(all_embeddings)\n",
        "\n",
        "    def align_prompt_embeddings(self, encoded_prompts, batch_size, window_size, x_patched):\n",
        "        \"\"\"Aligns encoded prompts with time series embeddings.\"\"\"\n",
        "        return encoded_prompts.unsqueeze(2).expand(-1, -1, x_patched.size(2), -1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class SupervisedSegmentationTask:\n",
        "    def __init__(self, model, train_loader, val_loader, config):\n",
        "        self.config = config\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "        self.optimizer = torch.optim.Adam(\n",
        "            self.model.parameters(),\n",
        "            lr=config['lr'],\n",
        "            weight_decay=config.get('weight_decay', 1e-4)\n",
        "        )\n",
        "        self.scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "            self.optimizer,\n",
        "            step_size=config.get('scheduler_step_size', 10),\n",
        "            gamma=config.get('scheduler_gamma', 0.7)\n",
        "        )\n",
        "\n",
        "        # Add gradient clipping\n",
        "        self.grad_clip = config.get('training', {}).get('gradient_clip', 1.0)\n",
        "\n",
        "        # Initialize best metrics\n",
        "        self.best_metrics = None\n",
        "        self.train_metrics = None\n",
        "        self.val_metrics = None\n",
        "\n",
        "    def train_epoch(self, epoch):\n",
        "        \"\"\"Training with improved metric handling and memory management\"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "        all_predictions = []\n",
        "        all_targets = []\n",
        "\n",
        "        # Use tqdm for progress tracking\n",
        "        pbar = tqdm(self.train_loader, desc=f\"Epoch {epoch+1}\")\n",
        "\n",
        "        for batch_idx, batch in enumerate(pbar):\n",
        "            try:\n",
        "                # Move data to device efficiently\n",
        "                inputs = batch['x_enc'].to(self.device, non_blocking=True)\n",
        "                prompts = batch['text']\n",
        "                targets = batch['labels'].to(self.device, non_blocking=True)\n",
        "\n",
        "                # Clear gradients\n",
        "                self.optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "                # Forward pass with autocast for mixed precision\n",
        "                with torch.amp.autocast(device_type=\"cuda\", enabled=True):\n",
        "                    outputs = self.model(inputs, prompts)\n",
        "                    loss = self.loss_fn(outputs, targets)\n",
        "\n",
        "                # Backward pass and optimization\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # Update metrics\n",
        "                predictions = torch.argmax(outputs, dim=1)\n",
        "                all_predictions.extend(predictions.cpu().numpy())\n",
        "                all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                num_batches += 1\n",
        "\n",
        "                # Update progress bar\n",
        "                if batch_idx % 10 == 0:\n",
        "                    metrics = self._calculate_metrics(all_predictions, all_targets)\n",
        "                    pbar.set_postfix(\n",
        "                        loss=f\"{loss.item():.4f}\",\n",
        "                        acc=f\"{metrics['accuracy']:.4f}\",\n",
        "                        f1=f\"{metrics['f1']:.4f}\"\n",
        "                    )\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                if \"out of memory\" in str(e):\n",
        "                    if hasattr(torch.cuda, 'empty_cache'):\n",
        "                        torch.cuda.empty_cache()\n",
        "                    logger.error(f\"GPU OOM in batch {batch_idx}. Try reducing batch size.\")\n",
        "                raise e\n",
        "\n",
        "        # Calculate final metrics\n",
        "        metrics = self._calculate_metrics(all_predictions, all_targets)\n",
        "        metrics['loss'] = total_loss / num_batches\n",
        "\n",
        "        # Store metrics\n",
        "        self.train_metrics = metrics\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def validate(self, loader):\n",
        "        \"\"\"Validate the model with comprehensive metrics\"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "        all_predictions = []\n",
        "        all_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in loader:\n",
        "                inputs = batch['x_enc'].to(self.device)\n",
        "                prompts = batch['text']\n",
        "                targets = batch['labels'].to(self.device)\n",
        "\n",
        "                outputs = self.model(inputs, prompts)\n",
        "                loss = self.loss_fn(outputs, targets)\n",
        "\n",
        "                predictions = torch.argmax(outputs, dim=1)\n",
        "                all_predictions.extend(predictions.cpu().numpy())\n",
        "                all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                num_batches += 1\n",
        "\n",
        "        metrics = self._calculate_metrics(all_predictions, all_targets)\n",
        "        metrics['loss'] = total_loss / num_batches\n",
        "\n",
        "        # Store validation metrics\n",
        "        self.val_metrics = metrics\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def train(self, epochs):\n",
        "        \"\"\"Train for multiple epochs with validation\"\"\"\n",
        "        best_loss = float('inf')\n",
        "        patience = self.config.get('training', {}).get('early_stopping_patience', 5)\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Train epoch\n",
        "            train_metrics = self.train_epoch(epoch)\n",
        "            logger.info(\n",
        "                f\"Epoch {epoch + 1}/{self.config['training']['epochs']}, \"\n",
        "                f\"Train Loss: {train_metrics['loss']:.4f}, \"\n",
        "                f\"Train Acc: {train_metrics['accuracy']:.4f}, \"\n",
        "                f\"Train Precision: {train_metrics['precision']:.4f}, \"\n",
        "                f\"Train Recall: {train_metrics['recall']:.4f}, \"\n",
        "                f\"Train F1: {train_metrics['f1']:.4f}\"\n",
        "            )\n",
        "\n",
        "            # Validate\n",
        "            val_metrics = self.validate(self.val_loader)\n",
        "            logger.info(\n",
        "                f\"Validation Loss: {val_metrics['loss']:.4f}, \"\n",
        "                f\"Validation Acc: {val_metrics['accuracy']:.4f}, \"\n",
        "                f\"Validation Precision: {val_metrics['precision']:.4f}, \"\n",
        "                f\"Validation Recall: {val_metrics['recall']:.4f}, \"\n",
        "                f\"Validation F1: {val_metrics['f1']:.4f}\"\n",
        "            )\n",
        "\n",
        "            # Early stopping check\n",
        "            if val_metrics['loss'] < best_loss:\n",
        "                best_loss = val_metrics['loss']\n",
        "                patience_counter = 0\n",
        "                self.best_metrics = val_metrics\n",
        "                self.save_checkpoint(f'checkpoints/best_model_epoch_{epoch}.pth')\n",
        "                logger.info(f\"Saved best model with val_loss: {val_metrics['loss']:.4f}\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    logger.info(\"Early stopping triggered\")\n",
        "                    break\n",
        "\n",
        "            self.scheduler.step()\n",
        "\n",
        "    def _calculate_metrics(self, predictions, targets):\n",
        "        \"\"\"Calculate metrics in a memory-efficient way\"\"\"\n",
        "        return {\n",
        "            'accuracy': accuracy_score(targets, predictions),\n",
        "            'precision': precision_score(targets, predictions, zero_division='warn'),\n",
        "            'recall': recall_score(targets, predictions, zero_division='warn'),\n",
        "            'f1': f1_score(targets, predictions, zero_division='warn')\n",
        "        }\n",
        "\n",
        "    def save_checkpoint(self, path):\n",
        "        \"\"\"Save model checkpoint with comprehensive metrics\"\"\"\n",
        "        checkpoint = {\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "            'config': self.config,\n",
        "            'metrics': {\n",
        "                'train_metrics': self.train_metrics,\n",
        "                'val_metrics': self.val_metrics,\n",
        "                'best_metrics': self.best_metrics\n",
        "            }\n",
        "        }\n",
        "        try:\n",
        "            torch.save(checkpoint, path)\n",
        "            logger.info(f\"Successfully saved checkpoint to {path}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error saving checkpoint: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def load_checkpoint(self, path):\n",
        "        \"\"\"Load model checkpoint with metrics\"\"\"\n",
        "        try:\n",
        "            checkpoint = torch.load(path)\n",
        "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "            self.train_metrics = checkpoint.get('metrics', {}).get('train_metrics')\n",
        "            self.val_metrics = checkpoint.get('metrics', {}).get('val_metrics')\n",
        "            self.best_metrics = checkpoint.get('metrics', {}).get('best_metrics')\n",
        "            logger.info(f\"Successfully loaded checkpoint from {path}\")\n",
        "            return checkpoint['metrics']\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading checkpoint: {str(e)}\")\n",
        "            raise"
      ],
      "metadata": {
        "id": "aJZZuo2Ha42y"
      },
      "id": "aJZZuo2Ha42y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d80d6fdf-22e6-43ca-9e71-154b93eb42c0",
      "metadata": {
        "id": "d80d6fdf-22e6-43ca-9e71-154b93eb42c0"
      },
      "source": [
        "Visualization Function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "\n",
        "def plot_financial_predictions(model, dataset, predictions, targets=None, window_size=5):\n",
        "    \"\"\"\n",
        "    Create a comprehensive visualization of financial predictions.\n",
        "    \"\"\"\n",
        "    sns.set_theme()\n",
        "    plt.figure(figsize=(20, 15))\n",
        "\n",
        "    # Convert dates to datetime\n",
        "    dates = pd.to_datetime(dataset.dates)\n",
        "\n",
        "    # 1. Price Movement Plot\n",
        "    ax1 = plt.subplot(3, 2, 1)\n",
        "    close_prices = dataset.features[:, 3, 0]\n",
        "    ax1.plot(dates, close_prices, label='Close Price', color='blue', linewidth=1)\n",
        "\n",
        "    # Add validation period shading\n",
        "    # val_start_idx = int(len(dates) * 0.8)  # 80-20 split\n",
        "    # ax1.axvspan(dates[val_start_idx], dates[-1],\n",
        "    #             color='yellow', alpha=0.1, label='Validation Period')\n",
        "\n",
        "    # # Highlight predictions\n",
        "    # for i, pred in enumerate(predictions):\n",
        "    #     if i+window_size < len(dates):\n",
        "    #         color = 'green' if pred == 1 else 'red'\n",
        "    #         ax1.axvspan(dates[i], dates[i+window_size], alpha=0.2, color=color)\n",
        "\n",
        "    val_start_idx = int(len(dates) * 0.8)  # 80-20 split\n",
        "    val_dates = dates[val_start_idx:]\n",
        "    val_predictions = predictions[-len(val_dates):]  # Only take predictions for validation period\n",
        "\n",
        "    for i, pred in enumerate(val_predictions):\n",
        "        if i+window_size < len(val_dates):\n",
        "            color = 'green' if pred == 1 else 'red'\n",
        "            ax1.axvspan(val_dates[i], val_dates[i+window_size], alpha=0.2, color=color)\n",
        "\n",
        "    ax1.set_title('Price Movement with Predictions', fontsize=12, pad=10)\n",
        "    ax1.set_xlabel('Date', fontsize=10)\n",
        "    ax1.set_ylabel('Normalized Price', fontsize=10)\n",
        "\n",
        "    # Updated date locator and formatter\n",
        "    ax1.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "    ax1.xaxis.set_minor_locator(mdates.DayLocator())\n",
        "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "    ax1.legend()\n",
        "\n",
        "    # 2. Confusion Matrix\n",
        "    if targets is not None:\n",
        "        ax2 = plt.subplot(3, 2, 2)\n",
        "        cm = confusion_matrix(targets, predictions)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax2)\n",
        "        ax2.set_title('Confusion Matrix', fontsize=12, pad=10)\n",
        "        ax2.set_xlabel('Predicted', fontsize=10)\n",
        "        ax2.set_ylabel('Actual', fontsize=10)\n",
        "\n",
        "    # 3. Daily Returns Distribution\n",
        "    ax3 = plt.subplot(3, 2, 3)\n",
        "    returns = np.diff(close_prices) / close_prices[:-1]\n",
        "    sns.histplot(returns, kde=True, ax=ax3, bins=50)\n",
        "    ax3.set_title('Distribution of Daily Returns', fontsize=12, pad=10)\n",
        "    ax3.set_xlabel('Returns', fontsize=10)\n",
        "    ax3.set_ylabel('Count', fontsize=10)\n",
        "\n",
        "    # 4. Volume Profile\n",
        "    ax4 = plt.subplot(3, 2, 4)\n",
        "    volumes = dataset.features[:, 4, 0]\n",
        "    ax4.bar(dates, volumes, alpha=0.5, width=1)\n",
        "    ax4.set_title('Trading Volume Profile', fontsize=12, pad=10)\n",
        "    ax4.set_xlabel('Date', fontsize=10)\n",
        "    ax4.set_ylabel('Normalized Volume', fontsize=10)\n",
        "\n",
        "    # Updated date locator and formatter for volume plot\n",
        "    ax4.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "    ax4.xaxis.set_minor_locator(mdates.DayLocator())\n",
        "    ax4.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "    plt.setp(ax4.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "\n",
        "    # 5. Model Confidence\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        ax5 = plt.subplot(3, 2, 5)\n",
        "        probas = model.predict_proba(dataset)\n",
        "        ax5.hist(probas[:, 1], bins=50)\n",
        "        ax5.set_title('Model Prediction Confidence', fontsize=12, pad=10)\n",
        "        ax5.set_xlabel('Probability of Upward Movement', fontsize=10)\n",
        "        ax5.set_ylabel('Count', fontsize=10)\n",
        "\n",
        "    # 6. Performance Metrics\n",
        "    if targets is not None:\n",
        "        ax6 = plt.subplot(3, 2, 6)\n",
        "        report = classification_report(targets, predictions, target_names=['Down', 'Up'])\n",
        "        ax6.text(0.1, 0.1, report, fontsize=10, family='monospace')\n",
        "        ax6.axis('off')\n",
        "        ax6.set_title('Classification Metrics', fontsize=12, pad=10)\n",
        "\n",
        "    plt.tight_layout(pad=3.0, h_pad=3.0, w_pad=3.0)\n",
        "    return plt.gcf()\n",
        "\n",
        "def track_training_metrics(metrics_history):\n",
        "    \"\"\"\n",
        "    Function to track and plot training metrics over epochs.\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot training and validation loss\n",
        "    ax1.plot(metrics_history['train_loss'], label='Training Loss')\n",
        "    ax1.plot(metrics_history['val_loss'], label='Validation Loss')\n",
        "    ax1.set_title('Loss Over Epochs')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "\n",
        "    # Plot accuracies\n",
        "    ax2.plot(metrics_history['train_accuracy'], label='Training Accuracy')\n",
        "    ax2.plot(metrics_history['val_accuracy'], label='Validation Accuracy')\n",
        "    ax2.set_title('Accuracy Over Epochs')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig"
      ],
      "metadata": {
        "id": "bXsjbZykkpLG"
      },
      "id": "bXsjbZykkpLG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bLEOHtD4vm_h",
      "metadata": {
        "id": "bLEOHtD4vm_h"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "Path('checkpoints').mkdir(exist_ok=True)\n",
        "\n",
        "def create_temporal_split(dataset, val_ratio=0.2, window_size=5):\n",
        "    \"\"\"Create temporal train/validation split with clear date boundaries\"\"\"\n",
        "    total_size = len(dataset)\n",
        "    split_idx = int((1 - val_ratio) * total_size)\n",
        "    split_idx = split_idx - window_size\n",
        "\n",
        "    train_indices = list(range(split_idx))\n",
        "    val_indices = list(range(split_idx + window_size, total_size))\n",
        "\n",
        "    train_dataset = Subset(dataset, train_indices)\n",
        "    val_dataset = Subset(dataset, val_indices)\n",
        "\n",
        "    # Print date ranges\n",
        "    train_start = dataset.dates[0]\n",
        "    train_end = dataset.dates[split_idx]\n",
        "    val_start = dataset.dates[split_idx + window_size]\n",
        "    val_end = dataset.dates[-1]\n",
        "\n",
        "    print(f\"Training period: {train_start} to {train_end}\")\n",
        "    print(f\"Validation period: {val_start} to {val_end}\")\n",
        "\n",
        "    return train_dataset, val_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hTx2iBdltsBP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTx2iBdltsBP",
        "outputId": "0d1f0fc7-a3d2-4a5a-829d-f7590a8f6d0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing time series data...\n",
            "Loading text data...\n",
            "Merging datasets...\n",
            "Processing features...\n",
            "Dataset shapes:\n",
            "Features: (334, 5, 390)\n",
            "Number of text samples: 334\n",
            "Training period: 2021-01-04 to 2022-01-12\n",
            "Validation period: 2022-01-20 to 2022-04-29\n",
            "Input features shape: (334, 5, 390)\n",
            "Patch length: 30\n",
            "Stride: 15\n",
            "Final projection dim: 16640\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Epoch 1:   0%|          | 0/17 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1:   6%|▌         | 1/17 [00:06<01:42,  6.42s/it, acc=0.7500, f1=0.8571, loss=0.5538]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:  12%|█▏        | 2/17 [00:11<01:26,  5.74s/it, acc=0.7500, f1=0.8571, loss=0.5538]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:  18%|█▊        | 3/17 [00:17<01:18,  5.62s/it, acc=0.7500, f1=0.8571, loss=0.5538]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:  24%|██▎       | 4/17 [00:22<01:11,  5.48s/it, acc=0.7500, f1=0.8571, loss=0.5538]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:  29%|██▉       | 5/17 [00:27<01:04,  5.35s/it, acc=0.7500, f1=0.8571, loss=0.5538]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:  35%|███▌      | 6/17 [00:33<01:00,  5.51s/it, acc=0.7500, f1=0.8571, loss=0.5538]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:  41%|████      | 7/17 [00:38<00:54,  5.46s/it, acc=0.7500, f1=0.8571, loss=0.5538]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:  47%|████▋     | 8/17 [00:43<00:47,  5.33s/it, acc=0.7500, f1=0.8571, loss=0.5538]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:  53%|█████▎    | 9/17 [00:48<00:42,  5.25s/it, acc=0.7500, f1=0.8571, loss=0.5538]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:  59%|█████▉    | 10/17 [00:53<00:36,  5.17s/it, acc=0.7500, f1=0.8571, loss=0.5538]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1:  65%|██████▍   | 11/17 [00:58<00:30,  5.13s/it, acc=0.5000, f1=0.5319, loss=0.6580]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:  71%|███████   | 12/17 [01:04<00:25,  5.14s/it, acc=0.5000, f1=0.5319, loss=0.6580]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:  76%|███████▋  | 13/17 [01:09<00:20,  5.22s/it, acc=0.5000, f1=0.5319, loss=0.6580]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:  82%|████████▏ | 14/17 [01:14<00:15,  5.29s/it, acc=0.5000, f1=0.5319, loss=0.6580]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:  88%|████████▊ | 15/17 [01:19<00:10,  5.20s/it, acc=0.5000, f1=0.5319, loss=0.6580]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:  94%|█████████▍| 16/17 [01:24<00:05,  5.11s/it, acc=0.5000, f1=0.5319, loss=0.6580]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([3, 5, 5, 390])\n",
            "After patch embedding: torch.Size([3, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 17/17 [01:25<00:00,  5.05s/it, acc=0.5000, f1=0.5319, loss=0.6580]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([3, 5, 26, 128])\n",
            "Before encoder: torch.Size([15, 26, 128])\n",
            "After encoder: torch.Size([15, 26, 128])\n",
            "After reshaping: torch.Size([3, 5, 3328])\n",
            "Final shape before projection: torch.Size([3, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([2, 5, 5, 390])\n",
            "After patch embedding: torch.Size([2, 5, 26, 128])\n",
            "After text fusion: torch.Size([2, 5, 26, 128])\n",
            "Before encoder: torch.Size([10, 26, 128])\n",
            "After encoder: torch.Size([10, 26, 128])\n",
            "After reshaping: torch.Size([2, 5, 3328])\n",
            "Final shape before projection: torch.Size([2, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 2:   0%|          | 0/17 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2:   6%|▌         | 1/17 [00:05<01:25,  5.37s/it, acc=0.5625, f1=0.7200, loss=0.9532]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 2:  12%|█▏        | 2/17 [00:10<01:20,  5.36s/it, acc=0.5625, f1=0.7200, loss=0.9532]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 2:  18%|█▊        | 3/17 [00:15<01:13,  5.25s/it, acc=0.5625, f1=0.7200, loss=0.9532]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 2:  24%|██▎       | 4/17 [00:20<01:06,  5.12s/it, acc=0.5625, f1=0.7200, loss=0.9532]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 2:  29%|██▉       | 5/17 [00:25<01:01,  5.15s/it, acc=0.5625, f1=0.7200, loss=0.9532]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 2:  35%|███▌      | 6/17 [00:31<00:56,  5.15s/it, acc=0.5625, f1=0.7200, loss=0.9532]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 2:  41%|████      | 7/17 [00:35<00:50,  5.03s/it, acc=0.5625, f1=0.7200, loss=0.9532]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 2:  47%|████▋     | 8/17 [00:40<00:44,  4.98s/it, acc=0.5625, f1=0.7200, loss=0.9532]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 2:  53%|█████▎    | 9/17 [00:46<00:40,  5.12s/it, acc=0.5625, f1=0.7200, loss=0.9532]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 2:  59%|█████▉    | 10/17 [00:50<00:34,  4.98s/it, acc=0.5625, f1=0.7200, loss=0.9532]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2:  65%|██████▍   | 11/17 [00:55<00:29,  4.98s/it, acc=0.5170, f1=0.6083, loss=0.8765]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 2:  71%|███████   | 12/17 [01:01<00:25,  5.08s/it, acc=0.5170, f1=0.6083, loss=0.8765]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 2:  76%|███████▋  | 13/17 [01:06<00:21,  5.27s/it, acc=0.5170, f1=0.6083, loss=0.8765]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 2:  82%|████████▏ | 14/17 [01:12<00:15,  5.24s/it, acc=0.5170, f1=0.6083, loss=0.8765]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 2:  88%|████████▊ | 15/17 [01:17<00:10,  5.17s/it, acc=0.5170, f1=0.6083, loss=0.8765]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 2:  94%|█████████▍| 16/17 [01:21<00:05,  5.10s/it, acc=0.5170, f1=0.6083, loss=0.8765]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([3, 5, 5, 390])\n",
            "After patch embedding: torch.Size([3, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 17/17 [01:22<00:00,  4.88s/it, acc=0.5170, f1=0.6083, loss=0.8765]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([3, 5, 26, 128])\n",
            "Before encoder: torch.Size([15, 26, 128])\n",
            "After encoder: torch.Size([15, 26, 128])\n",
            "After reshaping: torch.Size([3, 5, 3328])\n",
            "Final shape before projection: torch.Size([3, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([2, 5, 5, 390])\n",
            "After patch embedding: torch.Size([2, 5, 26, 128])\n",
            "After text fusion: torch.Size([2, 5, 26, 128])\n",
            "Before encoder: torch.Size([10, 26, 128])\n",
            "After encoder: torch.Size([10, 26, 128])\n",
            "After reshaping: torch.Size([2, 5, 3328])\n",
            "Final shape before projection: torch.Size([2, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 3:   0%|          | 0/17 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3:   6%|▌         | 1/17 [00:04<01:19,  4.98s/it, acc=0.6875, f1=0.7619, loss=0.4824]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 3:  12%|█▏        | 2/17 [00:10<01:15,  5.03s/it, acc=0.6875, f1=0.7619, loss=0.4824]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 3:  18%|█▊        | 3/17 [00:15<01:10,  5.02s/it, acc=0.6875, f1=0.7619, loss=0.4824]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 3:  24%|██▎       | 4/17 [00:20<01:07,  5.19s/it, acc=0.6875, f1=0.7619, loss=0.4824]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 3:  29%|██▉       | 5/17 [00:25<01:01,  5.16s/it, acc=0.6875, f1=0.7619, loss=0.4824]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 3:  35%|███▌      | 6/17 [00:31<00:57,  5.25s/it, acc=0.6875, f1=0.7619, loss=0.4824]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 3:  41%|████      | 7/17 [00:36<00:51,  5.18s/it, acc=0.6875, f1=0.7619, loss=0.4824]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 3:  47%|████▋     | 8/17 [00:41<00:46,  5.16s/it, acc=0.6875, f1=0.7619, loss=0.4824]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 3:  53%|█████▎    | 9/17 [00:46<00:41,  5.20s/it, acc=0.6875, f1=0.7619, loss=0.4824]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 3:  59%|█████▉    | 10/17 [00:51<00:35,  5.12s/it, acc=0.6875, f1=0.7619, loss=0.4824]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3:  65%|██████▍   | 11/17 [00:56<00:30,  5.12s/it, acc=0.7159, f1=0.7788, loss=0.5469]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 3:  71%|███████   | 12/17 [01:01<00:25,  5.15s/it, acc=0.7159, f1=0.7788, loss=0.5469]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 3:  76%|███████▋  | 13/17 [01:06<00:20,  5.12s/it, acc=0.7159, f1=0.7788, loss=0.5469]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 3:  82%|████████▏ | 14/17 [01:11<00:15,  5.05s/it, acc=0.7159, f1=0.7788, loss=0.5469]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 3:  88%|████████▊ | 15/17 [01:16<00:10,  5.04s/it, acc=0.7159, f1=0.7788, loss=0.5469]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 3:  94%|█████████▍| 16/17 [01:21<00:05,  5.05s/it, acc=0.7159, f1=0.7788, loss=0.5469]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([3, 5, 5, 390])\n",
            "After patch embedding: torch.Size([3, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 17/17 [01:22<00:00,  4.87s/it, acc=0.7159, f1=0.7788, loss=0.5469]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([3, 5, 26, 128])\n",
            "Before encoder: torch.Size([15, 26, 128])\n",
            "After encoder: torch.Size([15, 26, 128])\n",
            "After reshaping: torch.Size([3, 5, 3328])\n",
            "Final shape before projection: torch.Size([3, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([2, 5, 5, 390])\n",
            "After patch embedding: torch.Size([2, 5, 26, 128])\n",
            "After text fusion: torch.Size([2, 5, 26, 128])\n",
            "Before encoder: torch.Size([10, 26, 128])\n",
            "After encoder: torch.Size([10, 26, 128])\n",
            "After reshaping: torch.Size([2, 5, 3328])\n",
            "Final shape before projection: torch.Size([2, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 4:   0%|          | 0/17 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4:   6%|▌         | 1/17 [00:05<01:26,  5.39s/it, acc=0.8125, f1=0.8421, loss=0.5027]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 4:  12%|█▏        | 2/17 [00:09<01:12,  4.84s/it, acc=0.8125, f1=0.8421, loss=0.5027]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 4:  18%|█▊        | 3/17 [00:14<01:07,  4.84s/it, acc=0.8125, f1=0.8421, loss=0.5027]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 4:  24%|██▎       | 4/17 [00:19<01:04,  4.95s/it, acc=0.8125, f1=0.8421, loss=0.5027]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 4:  29%|██▉       | 5/17 [00:24<01:00,  5.04s/it, acc=0.8125, f1=0.8421, loss=0.5027]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 4:  35%|███▌      | 6/17 [00:30<00:56,  5.10s/it, acc=0.8125, f1=0.8421, loss=0.5027]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 4:  41%|████      | 7/17 [00:35<00:51,  5.14s/it, acc=0.8125, f1=0.8421, loss=0.5027]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 4:  47%|████▋     | 8/17 [00:40<00:46,  5.16s/it, acc=0.8125, f1=0.8421, loss=0.5027]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 4:  53%|█████▎    | 9/17 [00:46<00:41,  5.24s/it, acc=0.8125, f1=0.8421, loss=0.5027]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 4:  59%|█████▉    | 10/17 [00:51<00:36,  5.23s/it, acc=0.8125, f1=0.8421, loss=0.5027]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4:  65%|██████▍   | 11/17 [00:56<00:31,  5.20s/it, acc=0.7784, f1=0.8282, loss=0.5101]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 4:  71%|███████   | 12/17 [01:01<00:25,  5.16s/it, acc=0.7784, f1=0.8282, loss=0.5101]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 4:  76%|███████▋  | 13/17 [01:06<00:20,  5.16s/it, acc=0.7784, f1=0.8282, loss=0.5101]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 4:  82%|████████▏ | 14/17 [01:11<00:15,  5.20s/it, acc=0.7784, f1=0.8282, loss=0.5101]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 4:  88%|████████▊ | 15/17 [01:17<00:10,  5.23s/it, acc=0.7784, f1=0.8282, loss=0.5101]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 4:  94%|█████████▍| 16/17 [01:22<00:05,  5.17s/it, acc=0.7784, f1=0.8282, loss=0.5101]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([3, 5, 5, 390])\n",
            "After patch embedding: torch.Size([3, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 17/17 [01:23<00:00,  4.90s/it, acc=0.7784, f1=0.8282, loss=0.5101]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([3, 5, 26, 128])\n",
            "Before encoder: torch.Size([15, 26, 128])\n",
            "After encoder: torch.Size([15, 26, 128])\n",
            "After reshaping: torch.Size([3, 5, 3328])\n",
            "Final shape before projection: torch.Size([3, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([2, 5, 5, 390])\n",
            "After patch embedding: torch.Size([2, 5, 26, 128])\n",
            "After text fusion: torch.Size([2, 5, 26, 128])\n",
            "Before encoder: torch.Size([10, 26, 128])\n",
            "After encoder: torch.Size([10, 26, 128])\n",
            "After reshaping: torch.Size([2, 5, 3328])\n",
            "Final shape before projection: torch.Size([2, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 5:   0%|          | 0/17 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5:   6%|▌         | 1/17 [00:05<01:25,  5.34s/it, acc=0.9375, f1=0.9600, loss=0.2802]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 5:  12%|█▏        | 2/17 [00:10<01:19,  5.27s/it, acc=0.9375, f1=0.9600, loss=0.2802]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 5:  18%|█▊        | 3/17 [00:15<01:13,  5.25s/it, acc=0.9375, f1=0.9600, loss=0.2802]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 5:  24%|██▎       | 4/17 [00:20<01:07,  5.16s/it, acc=0.9375, f1=0.9600, loss=0.2802]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 5:  29%|██▉       | 5/17 [00:26<01:02,  5.18s/it, acc=0.9375, f1=0.9600, loss=0.2802]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 5:  35%|███▌      | 6/17 [00:31<00:56,  5.17s/it, acc=0.9375, f1=0.9600, loss=0.2802]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 5:  41%|████      | 7/17 [00:36<00:50,  5.09s/it, acc=0.9375, f1=0.9600, loss=0.2802]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 5:  47%|████▋     | 8/17 [00:41<00:45,  5.10s/it, acc=0.9375, f1=0.9600, loss=0.2802]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 5:  53%|█████▎    | 9/17 [00:46<00:41,  5.13s/it, acc=0.9375, f1=0.9600, loss=0.2802]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 5:  59%|█████▉    | 10/17 [00:51<00:35,  5.03s/it, acc=0.9375, f1=0.9600, loss=0.2802]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5:  65%|██████▍   | 11/17 [00:56<00:30,  5.13s/it, acc=0.7898, f1=0.8384, loss=0.4023]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 5:  71%|███████   | 12/17 [01:01<00:25,  5.14s/it, acc=0.7898, f1=0.8384, loss=0.4023]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 5:  76%|███████▋  | 13/17 [01:06<00:20,  5.12s/it, acc=0.7898, f1=0.8384, loss=0.4023]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 5:  82%|████████▏ | 14/17 [01:11<00:15,  5.05s/it, acc=0.7898, f1=0.8384, loss=0.4023]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 5:  88%|████████▊ | 15/17 [01:16<00:10,  5.08s/it, acc=0.7898, f1=0.8384, loss=0.4023]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 5:  94%|█████████▍| 16/17 [01:22<00:05,  5.11s/it, acc=0.7898, f1=0.8384, loss=0.4023]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([3, 5, 5, 390])\n",
            "After patch embedding: torch.Size([3, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 17/17 [01:23<00:00,  4.89s/it, acc=0.7898, f1=0.8384, loss=0.4023]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([3, 5, 26, 128])\n",
            "Before encoder: torch.Size([15, 26, 128])\n",
            "After encoder: torch.Size([15, 26, 128])\n",
            "After reshaping: torch.Size([3, 5, 3328])\n",
            "Final shape before projection: torch.Size([3, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([2, 5, 5, 390])\n",
            "After patch embedding: torch.Size([2, 5, 26, 128])\n",
            "After text fusion: torch.Size([2, 5, 26, 128])\n",
            "Before encoder: torch.Size([10, 26, 128])\n",
            "After encoder: torch.Size([10, 26, 128])\n",
            "After reshaping: torch.Size([2, 5, 3328])\n",
            "Final shape before projection: torch.Size([2, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 6:   0%|          | 0/17 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6:   6%|▌         | 1/17 [00:05<01:27,  5.46s/it, acc=0.7500, f1=0.8182, loss=0.3903]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 6:  12%|█▏        | 2/17 [00:10<01:21,  5.41s/it, acc=0.7500, f1=0.8182, loss=0.3903]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 6:  18%|█▊        | 3/17 [00:16<01:15,  5.39s/it, acc=0.7500, f1=0.8182, loss=0.3903]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 6:  24%|██▎       | 4/17 [00:21<01:08,  5.25s/it, acc=0.7500, f1=0.8182, loss=0.3903]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 6:  29%|██▉       | 5/17 [00:26<01:01,  5.17s/it, acc=0.7500, f1=0.8182, loss=0.3903]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 6:  35%|███▌      | 6/17 [00:31<00:56,  5.10s/it, acc=0.7500, f1=0.8182, loss=0.3903]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 6:  41%|████      | 7/17 [00:36<00:50,  5.07s/it, acc=0.7500, f1=0.8182, loss=0.3903]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 6:  47%|████▋     | 8/17 [00:41<00:45,  5.04s/it, acc=0.7500, f1=0.8182, loss=0.3903]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 6:  53%|█████▎    | 9/17 [00:46<00:40,  5.11s/it, acc=0.7500, f1=0.8182, loss=0.3903]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 6:  59%|█████▉    | 10/17 [00:51<00:35,  5.02s/it, acc=0.7500, f1=0.8182, loss=0.3903]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6:  65%|██████▍   | 11/17 [00:56<00:30,  5.02s/it, acc=0.7955, f1=0.8302, loss=0.4927]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 6:  71%|███████   | 12/17 [01:01<00:25,  5.01s/it, acc=0.7955, f1=0.8302, loss=0.4927]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 6:  76%|███████▋  | 13/17 [01:06<00:20,  5.05s/it, acc=0.7955, f1=0.8302, loss=0.4927]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 6:  82%|████████▏ | 14/17 [01:11<00:15,  5.06s/it, acc=0.7955, f1=0.8302, loss=0.4927]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 6:  88%|████████▊ | 15/17 [01:17<00:10,  5.19s/it, acc=0.7955, f1=0.8302, loss=0.4927]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 6:  94%|█████████▍| 16/17 [01:22<00:05,  5.17s/it, acc=0.7955, f1=0.8302, loss=0.4927]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([3, 5, 5, 390])\n",
            "After patch embedding: torch.Size([3, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 17/17 [01:23<00:00,  4.89s/it, acc=0.7955, f1=0.8302, loss=0.4927]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([3, 5, 26, 128])\n",
            "Before encoder: torch.Size([15, 26, 128])\n",
            "After encoder: torch.Size([15, 26, 128])\n",
            "After reshaping: torch.Size([3, 5, 3328])\n",
            "Final shape before projection: torch.Size([3, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([2, 5, 5, 390])\n",
            "After patch embedding: torch.Size([2, 5, 26, 128])\n",
            "After text fusion: torch.Size([2, 5, 26, 128])\n",
            "Before encoder: torch.Size([10, 26, 128])\n",
            "After encoder: torch.Size([10, 26, 128])\n",
            "After reshaping: torch.Size([2, 5, 3328])\n",
            "Final shape before projection: torch.Size([2, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 7:   0%|          | 0/17 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7:   6%|▌         | 1/17 [00:05<01:25,  5.33s/it, acc=0.8750, f1=0.9000, loss=0.2672]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 7:  12%|█▏        | 2/17 [00:10<01:19,  5.29s/it, acc=0.8750, f1=0.9000, loss=0.2672]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 7:  18%|█▊        | 3/17 [00:15<01:12,  5.19s/it, acc=0.8750, f1=0.9000, loss=0.2672]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 7:  24%|██▎       | 4/17 [00:20<01:05,  5.07s/it, acc=0.8750, f1=0.9000, loss=0.2672]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 7:  29%|██▉       | 5/17 [00:26<01:02,  5.22s/it, acc=0.8750, f1=0.9000, loss=0.2672]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 7:  35%|███▌      | 6/17 [00:31<00:57,  5.24s/it, acc=0.8750, f1=0.9000, loss=0.2672]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 7:  41%|████      | 7/17 [00:36<00:51,  5.18s/it, acc=0.8750, f1=0.9000, loss=0.2672]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 7:  47%|████▋     | 8/17 [00:41<00:46,  5.11s/it, acc=0.8750, f1=0.9000, loss=0.2672]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 7:  53%|█████▎    | 9/17 [00:46<00:41,  5.17s/it, acc=0.8750, f1=0.9000, loss=0.2672]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 7:  59%|█████▉    | 10/17 [00:51<00:35,  5.12s/it, acc=0.8750, f1=0.9000, loss=0.2672]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7:  65%|██████▍   | 11/17 [00:56<00:30,  5.14s/it, acc=0.8068, f1=0.8426, loss=0.4434]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 7:  71%|███████   | 12/17 [01:02<00:25,  5.17s/it, acc=0.8068, f1=0.8426, loss=0.4434]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 7:  76%|███████▋  | 13/17 [01:06<00:20,  5.07s/it, acc=0.8068, f1=0.8426, loss=0.4434]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 7:  82%|████████▏ | 14/17 [01:12<00:15,  5.12s/it, acc=0.8068, f1=0.8426, loss=0.4434]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 7:  88%|████████▊ | 15/17 [01:17<00:10,  5.09s/it, acc=0.8068, f1=0.8426, loss=0.4434]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 7:  94%|█████████▍| 16/17 [01:22<00:05,  5.05s/it, acc=0.8068, f1=0.8426, loss=0.4434]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([3, 5, 5, 390])\n",
            "After patch embedding: torch.Size([3, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 17/17 [01:23<00:00,  4.89s/it, acc=0.8068, f1=0.8426, loss=0.4434]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([3, 5, 26, 128])\n",
            "Before encoder: torch.Size([15, 26, 128])\n",
            "After encoder: torch.Size([15, 26, 128])\n",
            "After reshaping: torch.Size([3, 5, 3328])\n",
            "Final shape before projection: torch.Size([3, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([2, 5, 5, 390])\n",
            "After patch embedding: torch.Size([2, 5, 26, 128])\n",
            "After text fusion: torch.Size([2, 5, 26, 128])\n",
            "Before encoder: torch.Size([10, 26, 128])\n",
            "After encoder: torch.Size([10, 26, 128])\n",
            "After reshaping: torch.Size([2, 5, 3328])\n",
            "Final shape before projection: torch.Size([2, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 8:   0%|          | 0/17 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8:   6%|▌         | 1/17 [00:05<01:23,  5.22s/it, acc=0.8750, f1=0.8571, loss=0.3789]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 8:  12%|█▏        | 2/17 [00:10<01:16,  5.13s/it, acc=0.8750, f1=0.8571, loss=0.3789]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 8:  18%|█▊        | 3/17 [00:15<01:13,  5.26s/it, acc=0.8750, f1=0.8571, loss=0.3789]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 8:  24%|██▎       | 4/17 [00:20<01:07,  5.17s/it, acc=0.8750, f1=0.8571, loss=0.3789]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 8:  29%|██▉       | 5/17 [00:26<01:03,  5.29s/it, acc=0.8750, f1=0.8571, loss=0.3789]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 8:  35%|███▌      | 6/17 [00:31<00:57,  5.24s/it, acc=0.8750, f1=0.8571, loss=0.3789]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 8:  41%|████      | 7/17 [00:36<00:52,  5.23s/it, acc=0.8750, f1=0.8571, loss=0.3789]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 8:  47%|████▋     | 8/17 [00:42<00:47,  5.30s/it, acc=0.8750, f1=0.8571, loss=0.3789]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 8:  53%|█████▎    | 9/17 [00:47<00:42,  5.26s/it, acc=0.8750, f1=0.8571, loss=0.3789]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 8:  59%|█████▉    | 10/17 [00:52<00:35,  5.14s/it, acc=0.8750, f1=0.8571, loss=0.3789]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8:  65%|██████▍   | 11/17 [00:57<00:30,  5.16s/it, acc=0.8409, f1=0.8704, loss=0.6256]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 8:  71%|███████   | 12/17 [01:02<00:25,  5.13s/it, acc=0.8409, f1=0.8704, loss=0.6256]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 8:  76%|███████▋  | 13/17 [01:07<00:20,  5.03s/it, acc=0.8409, f1=0.8704, loss=0.6256]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 8:  82%|████████▏ | 14/17 [01:12<00:15,  5.08s/it, acc=0.8409, f1=0.8704, loss=0.6256]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 8:  88%|████████▊ | 15/17 [01:17<00:10,  5.12s/it, acc=0.8409, f1=0.8704, loss=0.6256]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 8:  94%|█████████▍| 16/17 [01:22<00:05,  5.06s/it, acc=0.8409, f1=0.8704, loss=0.6256]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([3, 5, 5, 390])\n",
            "After patch embedding: torch.Size([3, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 17/17 [01:23<00:00,  4.91s/it, acc=0.8409, f1=0.8704, loss=0.6256]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([3, 5, 26, 128])\n",
            "Before encoder: torch.Size([15, 26, 128])\n",
            "After encoder: torch.Size([15, 26, 128])\n",
            "After reshaping: torch.Size([3, 5, 3328])\n",
            "Final shape before projection: torch.Size([3, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([2, 5, 5, 390])\n",
            "After patch embedding: torch.Size([2, 5, 26, 128])\n",
            "After text fusion: torch.Size([2, 5, 26, 128])\n",
            "Before encoder: torch.Size([10, 26, 128])\n",
            "After encoder: torch.Size([10, 26, 128])\n",
            "After reshaping: torch.Size([2, 5, 3328])\n",
            "Final shape before projection: torch.Size([2, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 9:   0%|          | 0/17 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9:   6%|▌         | 1/17 [00:05<01:21,  5.11s/it, acc=0.7500, f1=0.7778, loss=0.4022]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 9:  12%|█▏        | 2/17 [00:10<01:16,  5.12s/it, acc=0.7500, f1=0.7778, loss=0.4022]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 9:  18%|█▊        | 3/17 [00:15<01:11,  5.11s/it, acc=0.7500, f1=0.7778, loss=0.4022]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 9:  24%|██▎       | 4/17 [00:20<01:07,  5.17s/it, acc=0.7500, f1=0.7778, loss=0.4022]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 9:  29%|██▉       | 5/17 [00:25<01:02,  5.18s/it, acc=0.7500, f1=0.7778, loss=0.4022]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 9:  35%|███▌      | 6/17 [00:31<00:57,  5.24s/it, acc=0.7500, f1=0.7778, loss=0.4022]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 9:  41%|████      | 7/17 [00:36<00:52,  5.26s/it, acc=0.7500, f1=0.7778, loss=0.4022]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 9:  47%|████▋     | 8/17 [00:41<00:46,  5.16s/it, acc=0.7500, f1=0.7778, loss=0.4022]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 9:  53%|█████▎    | 9/17 [00:46<00:40,  5.11s/it, acc=0.7500, f1=0.7778, loss=0.4022]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 9:  59%|█████▉    | 10/17 [00:51<00:36,  5.15s/it, acc=0.7500, f1=0.7778, loss=0.4022]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9:  65%|██████▍   | 11/17 [00:56<00:30,  5.08s/it, acc=0.8920, f1=0.9005, loss=0.4318]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 9:  71%|███████   | 12/17 [01:01<00:25,  5.16s/it, acc=0.8920, f1=0.9005, loss=0.4318]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 9:  76%|███████▋  | 13/17 [01:07<00:20,  5.17s/it, acc=0.8920, f1=0.9005, loss=0.4318]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 9:  82%|████████▏ | 14/17 [01:12<00:15,  5.13s/it, acc=0.8920, f1=0.9005, loss=0.4318]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 9:  88%|████████▊ | 15/17 [01:17<00:10,  5.12s/it, acc=0.8920, f1=0.9005, loss=0.4318]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 9:  94%|█████████▍| 16/17 [01:22<00:05,  5.08s/it, acc=0.8920, f1=0.9005, loss=0.4318]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([3, 5, 5, 390])\n",
            "After patch embedding: torch.Size([3, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 17/17 [01:23<00:00,  4.89s/it, acc=0.8920, f1=0.9005, loss=0.4318]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([3, 5, 26, 128])\n",
            "Before encoder: torch.Size([15, 26, 128])\n",
            "After encoder: torch.Size([15, 26, 128])\n",
            "After reshaping: torch.Size([3, 5, 3328])\n",
            "Final shape before projection: torch.Size([3, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([2, 5, 5, 390])\n",
            "After patch embedding: torch.Size([2, 5, 26, 128])\n",
            "After text fusion: torch.Size([2, 5, 26, 128])\n",
            "Before encoder: torch.Size([10, 26, 128])\n",
            "After encoder: torch.Size([10, 26, 128])\n",
            "After reshaping: torch.Size([2, 5, 3328])\n",
            "Final shape before projection: torch.Size([2, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 10:   0%|          | 0/17 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10:   6%|▌         | 1/17 [00:05<01:32,  5.80s/it, acc=0.9375, f1=0.9474, loss=0.2698]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 10:  12%|█▏        | 2/17 [00:11<01:22,  5.51s/it, acc=0.9375, f1=0.9474, loss=0.2698]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 10:  18%|█▊        | 3/17 [00:16<01:13,  5.27s/it, acc=0.9375, f1=0.9474, loss=0.2698]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 10:  24%|██▎       | 4/17 [00:21<01:09,  5.32s/it, acc=0.9375, f1=0.9474, loss=0.2698]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 10:  29%|██▉       | 5/17 [00:26<01:02,  5.23s/it, acc=0.9375, f1=0.9474, loss=0.2698]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 10:  35%|███▌      | 6/17 [00:31<00:56,  5.17s/it, acc=0.9375, f1=0.9474, loss=0.2698]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 10:  41%|████      | 7/17 [00:36<00:51,  5.11s/it, acc=0.9375, f1=0.9474, loss=0.2698]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 10:  47%|████▋     | 8/17 [00:41<00:46,  5.20s/it, acc=0.9375, f1=0.9474, loss=0.2698]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 10:  53%|█████▎    | 9/17 [00:47<00:42,  5.26s/it, acc=0.9375, f1=0.9474, loss=0.2698]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 10:  59%|█████▉    | 10/17 [00:52<00:35,  5.10s/it, acc=0.9375, f1=0.9474, loss=0.2698]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10:  65%|██████▍   | 11/17 [00:57<00:30,  5.08s/it, acc=0.8750, f1=0.9043, loss=0.2887]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 10:  71%|███████   | 12/17 [01:02<00:25,  5.06s/it, acc=0.8750, f1=0.9043, loss=0.2887]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 10:  76%|███████▋  | 13/17 [01:07<00:20,  5.08s/it, acc=0.8750, f1=0.9043, loss=0.2887]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 10:  82%|████████▏ | 14/17 [01:12<00:15,  5.02s/it, acc=0.8750, f1=0.9043, loss=0.2887]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 10:  88%|████████▊ | 15/17 [01:16<00:09,  4.94s/it, acc=0.8750, f1=0.9043, loss=0.2887]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 10:  94%|█████████▍| 16/17 [01:22<00:05,  5.02s/it, acc=0.8750, f1=0.9043, loss=0.2887]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([3, 5, 5, 390])\n",
            "After patch embedding: torch.Size([3, 5, 26, 128])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 17/17 [01:23<00:00,  4.88s/it, acc=0.8750, f1=0.9043, loss=0.2887]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After text fusion: torch.Size([3, 5, 26, 128])\n",
            "Before encoder: torch.Size([15, 26, 128])\n",
            "After encoder: torch.Size([15, 26, 128])\n",
            "After reshaping: torch.Size([3, 5, 3328])\n",
            "Final shape before projection: torch.Size([3, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([2, 5, 5, 390])\n",
            "After patch embedding: torch.Size([2, 5, 26, 128])\n",
            "After text fusion: torch.Size([2, 5, 26, 128])\n",
            "Before encoder: torch.Size([10, 26, 128])\n",
            "After encoder: torch.Size([10, 26, 128])\n",
            "After reshaping: torch.Size([2, 5, 3328])\n",
            "Final shape before projection: torch.Size([2, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([16, 5, 5, 390])\n",
            "After patch embedding: torch.Size([16, 5, 26, 128])\n",
            "After text fusion: torch.Size([16, 5, 26, 128])\n",
            "Before encoder: torch.Size([80, 26, 128])\n",
            "After encoder: torch.Size([80, 26, 128])\n",
            "After reshaping: torch.Size([16, 5, 3328])\n",
            "Final shape before projection: torch.Size([16, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n",
            "Input shape: torch.Size([2, 5, 5, 390])\n",
            "After patch embedding: torch.Size([2, 5, 26, 128])\n",
            "After text fusion: torch.Size([2, 5, 26, 128])\n",
            "Before encoder: torch.Size([10, 26, 128])\n",
            "After encoder: torch.Size([10, 26, 128])\n",
            "After reshaping: torch.Size([2, 5, 3328])\n",
            "Final shape before projection: torch.Size([2, 16640])\n",
            "Projection weight shape: torch.Size([2, 16640])\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import torch\n",
        "    import logging\n",
        "    from torch.utils.data import DataLoader, random_split\n",
        "    import numpy as np\n",
        "    from pathlib import Path\n",
        "\n",
        "    # Set up logging with detailed configuration\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "        handlers=[\n",
        "            logging.FileHandler('training.log'),\n",
        "            logging.StreamHandler()\n",
        "        ]\n",
        "    )\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    # Set random seeds for reproducibility\n",
        "    def set_seed(seed=42):\n",
        "        torch.manual_seed(seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    set_seed(42)\n",
        "\n",
        "    # Enhanced configuration\n",
        "    config = {\n",
        "        'models': {\n",
        "            'patchtst': {\n",
        "                # Embedding dimensions\n",
        "                'd_model': 128,\n",
        "\n",
        "                # Patch settings for minute-level data\n",
        "                'patching': {\n",
        "                    'patch_len': 30,     # 30-minute patches\n",
        "                    'stride': 15,        # 15-minute stride\n",
        "                    'padding': 15        # Padding at sequence ends\n",
        "                },\n",
        "\n",
        "                # Transformer settings\n",
        "                'n_heads': 4,\n",
        "                'd_ff': 512,\n",
        "                'e_layers': 3,\n",
        "            }\n",
        "        },\n",
        "\n",
        "        # Training settings\n",
        "        'training': {\n",
        "            'dropout': 0.1,\n",
        "            'batch_size': 16,\n",
        "            'epochs': 10,\n",
        "            'gradient_clip': 1.0,\n",
        "            'early_stopping_patience': 5,\n",
        "            'validation_split': 0.2,\n",
        "        },\n",
        "\n",
        "        # Optimization settings\n",
        "        'lr': 1e-4,\n",
        "        'weight_decay': 1e-4,\n",
        "        'scheduler_step_size': 5,\n",
        "        'scheduler_gamma': 0.7,\n",
        "\n",
        "        # Model structure settings\n",
        "        'history_len': 5,\n",
        "        'max_seq_len': 390,\n",
        "        'num_features': 5,\n",
        "\n",
        "        # Text processing settings\n",
        "        'bert_model': \"bert-base-uncased\",\n",
        "        'max_text_length': 128,\n",
        "        'freeze_bert': True,\n",
        "        'bert_pooling': 'cls',\n",
        "\n",
        "        # Checkpointing\n",
        "        'checkpoint_dir': 'checkpoints',\n",
        "        'save_every': 1,\n",
        "    }\n",
        "\n",
        "    metrics_history = {\n",
        "    'train_loss': [], 'val_loss': [],\n",
        "    'train_accuracy': [], 'val_accuracy': [],\n",
        "    'train_f1': [], 'val_f1': [],\n",
        "    'train_precision': [], 'val_precision': [],\n",
        "    'train_recall': [], 'val_recall': []\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Create checkpoint directory\n",
        "        checkpoint_dir = Path(config['checkpoint_dir'])\n",
        "        checkpoint_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Create datasets with error handling\n",
        "        logger.info(\"Creating datasets...\")\n",
        "        try:\n",
        "            dataset = MultimodalFinancialDataset(\n",
        "                time_series_path='AAPL_train_data_aggregated.csv',\n",
        "                text_path='AAPL_tweets_train.csv',\n",
        "                window_size=config['history_len'],\n",
        "                max_len=config['max_seq_len']\n",
        "            )\n",
        "\n",
        "            # Normalize features\n",
        "            logger.info(\"Normalizing features...\")\n",
        "            dataset.normalize_features()\n",
        "\n",
        "            # Split dataset into train and validation\n",
        "            # val_size = int(len(dataset) * config['training']['validation_split'])\n",
        "            # train_size = len(dataset) - val_size\n",
        "            # train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "            # Create temporal train-validation split\n",
        "            train_dataset, val_dataset = create_temporal_split(\n",
        "                dataset=dataset,\n",
        "                val_ratio=config['training']['validation_split'],\n",
        "                window_size=config['history_len']\n",
        "            )\n",
        "\n",
        "            logger.info(f\"Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}\")\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating dataset: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "        # Create dataloaders with error handling\n",
        "        logger.info(\"Creating dataloaders...\")\n",
        "        try:\n",
        "            train_loader = DataLoader(\n",
        "                train_dataset,\n",
        "                batch_size=config['training']['batch_size'],\n",
        "                shuffle=True,\n",
        "                collate_fn=custom_collate_fn,\n",
        "                num_workers=2,\n",
        "                pin_memory=True\n",
        "            )\n",
        "\n",
        "            val_loader = DataLoader(\n",
        "                val_dataset,\n",
        "                batch_size=config['training']['batch_size'],\n",
        "                shuffle=False,\n",
        "                collate_fn=custom_collate_fn,\n",
        "                num_workers=2,\n",
        "                pin_memory=True\n",
        "            )\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating dataloaders: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "        # Initialize model\n",
        "        logger.info(\"Initializing model...\")\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        logger.info(f\"Using device: {device}\")\n",
        "\n",
        "        try:\n",
        "            model = PatchTSTWithBERT(\n",
        "                config=config,\n",
        "                dataset=dataset,\n",
        "                bert_model=config['bert_model']\n",
        "            ).to(device)\n",
        "\n",
        "            # Print model summary\n",
        "            total_params = sum(p.numel() for p in model.parameters())\n",
        "            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "            logger.info(f\"Total parameters: {total_params:,}\")\n",
        "            logger.info(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error initializing model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "        # Create training task\n",
        "        logger.info(\"Setting up training task...\")\n",
        "        task = UnsupervisedSegmentationTask(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            config=config\n",
        "        )\n",
        "\n",
        "        # Training loop with enhanced error handling and monitoring\n",
        "        logger.info(\"Starting training...\")\n",
        "        best_val_loss = float('inf')\n",
        "        early_stopping_counter = 0\n",
        "\n",
        "        try:\n",
        "            for epoch in range(config['training']['epochs']):\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "                # Train\n",
        "                train_metrics = task.train_epoch(epoch)\n",
        "\n",
        "                # Store training metrics with key mapping\n",
        "                for k, v in train_metrics.items():\n",
        "                    if k == 'accuracy':\n",
        "                        metrics_history['train_accuracy'].append(v)\n",
        "                    else:\n",
        "                        metrics_history[f'train_{k}'].append(v)\n",
        "\n",
        "                logger.info(f\"Epoch {epoch + 1}/{config['training']['epochs']}, \"\n",
        "                          f\"Train Loss: {train_metrics['loss']:.4f}, \"\n",
        "                          f\"Train Accuracy: {train_metrics['accuracy']:.4f}, \"  # Changed from Acc to Accuracy\n",
        "                          f\"Train F1: {train_metrics['f1']:.4f}\")\n",
        "\n",
        "                # Validate\n",
        "                val_metrics = task.validate(val_loader)\n",
        "\n",
        "                # Store validation metrics with key mapping\n",
        "                for k, v in val_metrics.items():\n",
        "                    if k == 'accuracy':\n",
        "                        metrics_history['val_accuracy'].append(v)\n",
        "                    else:\n",
        "                        metrics_history[f'val_{k}'].append(v)\n",
        "\n",
        "                # Save checkpoint if validation loss improved\n",
        "                if val_metrics['loss'] < best_val_loss:\n",
        "                    best_val_loss = val_metrics['loss']\n",
        "                    early_stopping_counter = 0\n",
        "\n",
        "                    # Let the task handle checkpoint saving\n",
        "                    task.save_checkpoint(checkpoint_dir / f'best_model_epoch_{epoch}.pth')\n",
        "                    logger.info(f\"Saved best model with val_loss: {val_metrics['loss']:.4f}\")\n",
        "                else:\n",
        "                    early_stopping_counter += 1\n",
        "\n",
        "                # Early stopping check\n",
        "                if early_stopping_counter >= config['training']['early_stopping_patience']:\n",
        "                    logger.info(\"Early stopping triggered\")\n",
        "                    break\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            logger.info(\"Training interrupted by user\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during training: {str(e)}\")\n",
        "            import traceback\n",
        "            logger.error(traceback.format_exc())\n",
        "\n",
        "        finally:\n",
        "            final_checkpoint_path = checkpoint_dir / 'final_model.pth'\n",
        "            task.save_checkpoint(final_checkpoint_path)\n",
        "            logger.info(f\"Final model saved to {final_checkpoint_path}\")\n",
        "\n",
        "\n",
        "        logger.info(\"Creating final visualizations...\")\n",
        "\n",
        "        # 1. Plot training history\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "        # Loss plot\n",
        "        axes[0,0].plot(metrics_history['train_loss'], label='Train')\n",
        "        axes[0,0].plot(metrics_history['val_loss'], label='Validation')\n",
        "        axes[0,0].set_title('Loss Over Epochs')\n",
        "        axes[0,0].set_xlabel('Epoch')\n",
        "        axes[0,0].set_ylabel('Loss')\n",
        "        axes[0,0].legend()\n",
        "\n",
        "        # Accuracy plot\n",
        "        axes[0,1].plot(metrics_history['train_accuracy'], label='Train')\n",
        "        axes[0,1].plot(metrics_history['val_accuracy'], label='Validation')\n",
        "        axes[0,1].set_title('Accuracy Over Epochs')\n",
        "        axes[0,1].set_xlabel('Epoch')\n",
        "        axes[0,1].set_ylabel('Accuracy')\n",
        "        axes[0,1].legend()\n",
        "\n",
        "        # F1 Score plot\n",
        "        axes[1,0].plot(metrics_history['train_f1'], label='Train')\n",
        "        axes[1,0].plot(metrics_history['val_f1'], label='Validation')\n",
        "        axes[1,0].set_title('F1 Score Over Epochs')\n",
        "        axes[1,0].set_xlabel('Epoch')\n",
        "        axes[1,0].set_ylabel('F1 Score')\n",
        "        axes[1,0].legend()\n",
        "\n",
        "        # Make predictions on validation set\n",
        "        model.eval()\n",
        "        # all_preds = []\n",
        "        # all_targets = []\n",
        "        # with torch.no_grad():\n",
        "        #     for batch in val_loader:\n",
        "        #         inputs = batch['x_enc'].to(device)\n",
        "        #         targets = batch['labels'].to(device)\n",
        "        #         outputs = model(inputs, batch['text'])\n",
        "        #         preds = torch.argmax(outputs, dim=1)\n",
        "        #         all_preds.extend(preds.cpu().numpy())\n",
        "        #         all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "        val_indices = []  # Keep track of validation indices\n",
        "        val_preds = []\n",
        "        val_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(val_loader):\n",
        "                inputs = batch['x_enc'].to(device)\n",
        "                targets = batch['labels'].to(device)\n",
        "                outputs = model(inputs, batch['text'])\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "                # Collect predictions and targets\n",
        "                val_preds.extend(preds.cpu().numpy())\n",
        "                val_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "                # Compute and store validation indices\n",
        "                batch_start_idx = batch_idx * config['training']['batch_size']\n",
        "                batch_end_idx = min((batch_idx + 1) * config['training']['batch_size'], len(val_dataset))\n",
        "                val_indices.extend(range(batch_start_idx, batch_end_idx))\n",
        "\n",
        "        # Confusion Matrix\n",
        "        cm = confusion_matrix(val_targets, val_preds)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', ax=axes[1,1])\n",
        "        axes[1,1].set_title('Confusion Matrix')\n",
        "        axes[1,1].set_xlabel('Predicted')\n",
        "        axes[1,1].set_ylabel('Actual')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('training_metrics.png')\n",
        "        plt.close()\n",
        "\n",
        "        # Save metrics to file\n",
        "        with open('training_metrics.txt', 'w') as f:\n",
        "            f.write(\"Final Training Metrics:\\n\")\n",
        "            for k, v in train_metrics.items():\n",
        "                f.write(f\"{k}: {v:.4f}\\n\")\n",
        "            f.write(\"\\nFinal Validation Metrics:\\n\")\n",
        "            for k, v in val_metrics.items():\n",
        "                f.write(f\"{k}: {v:.4f}\\n\")\n",
        "\n",
        "        # Plot example predictions\n",
        "        # fig = plot_financial_predictions(\n",
        "        #     model=model,\n",
        "        #     dataset=dataset,\n",
        "        #     predictions=all_preds,\n",
        "        #     targets=all_targets,\n",
        "        #     window_size=config['history_len']\n",
        "        # )\n",
        "        fig = plot_financial_predictions(\n",
        "          model=model,\n",
        "          dataset=dataset,\n",
        "          predictions=val_preds,\n",
        "          targets=val_targets,\n",
        "          window_size=config['history_len']\n",
        "      )\n",
        "        plt.savefig('predictions_visualization.png')\n",
        "        plt.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Fatal error: {str(e)}\")\n",
        "        import traceback\n",
        "        logger.error(traceback.format_exc())\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "emPQN4lQ1sSn"
      },
      "id": "emPQN4lQ1sSn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "450138d7-2cba-40de-9012-dc0f0c4380c0",
      "metadata": {
        "id": "450138d7-2cba-40de-9012-dc0f0c4380c0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d950594-efaa-4a20-8efd-7ab95353ae44",
      "metadata": {
        "id": "1d950594-efaa-4a20-8efd-7ab95353ae44"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
