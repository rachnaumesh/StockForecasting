{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U kaleido"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzX-u47HFApN",
        "outputId": "8f1e7a3c-7d1b-49f5-9825-81e66c971e30"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaleido in /usr/local/lib/python3.10/dist-packages (0.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pkg_resources\n",
        "pkg_resources.working_set.by_key.get('kaleido')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4VxVC-DFh8T",
        "outputId": "2d817b5e-f6cb-4573-84dc-4895aa70432e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-351c8780b838>:1: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  import pkg_resources\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "kaleido 0.2.1 (/usr/local/lib/python3.10/dist-packages)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hqRB51Nl60Ww"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch import Tensor\n",
        "import math\n",
        "from math import sqrt\n",
        "import logging\n",
        "from typing import Dict, List, Tuple\n",
        "import tqdm\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "import sys\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('forecasting.log'),\n",
        "        logging.StreamHandler(sys.stdout)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "YX0LvDiATwAZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TriangularCausalMask:\n",
        "    def __init__(self, B, L, device=\"cpu\"):\n",
        "        mask_shape = [B, 1, L, L]\n",
        "        with torch.no_grad():\n",
        "            self._mask = torch.triu(torch.ones(mask_shape, dtype=torch.bool), diagonal=1).to(device)\n",
        "\n",
        "    @property\n",
        "    def mask(self):\n",
        "        return self._mask\n",
        "\n",
        "class FullAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        mask_flag=True,\n",
        "        factor=5,\n",
        "        scale=None,\n",
        "        attention_dropout=0.1,\n",
        "        output_attention=False,\n",
        "    ):\n",
        "        super(FullAttention, self).__init__()\n",
        "        self.scale = scale\n",
        "        self.mask_flag = mask_flag\n",
        "        self.output_attention = output_attention\n",
        "        self.dropout = nn.Dropout(attention_dropout)\n",
        "\n",
        "    def forward(self, queries, keys, values, attn_mask):\n",
        "        B, L, H, E = queries.shape\n",
        "        _, S, _, D = values.shape\n",
        "        scale = self.scale or 1.0 / sqrt(E)\n",
        "\n",
        "        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n",
        "\n",
        "        if self.mask_flag:\n",
        "            if attn_mask is None:\n",
        "                attn_mask = TriangularCausalMask(B, L, device=queries.device)\n",
        "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
        "\n",
        "        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n",
        "        V = torch.einsum(\"bhls,bshd->blhd\", A, values)\n",
        "        if self.output_attention:\n",
        "            return V.contiguous(), A\n",
        "        else:\n",
        "            return V.contiguous(), None\n",
        "\n",
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, attention, d_model, n_heads, d_keys=None, d_values=None):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "        d_keys = d_keys or (d_model // n_heads)\n",
        "        d_values = d_values or (d_model // n_heads)\n",
        "\n",
        "        self.inner_attention = attention\n",
        "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
        "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
        "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
        "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, queries, keys, values, attn_mask):\n",
        "        B, L, _ = queries.shape\n",
        "        _, S, _ = keys.shape\n",
        "        H = self.n_heads\n",
        "\n",
        "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
        "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
        "        values = self.value_projection(values).view(B, S, H, -1)\n",
        "\n",
        "        out, attn = self.inner_attention(queries, keys, values, attn_mask)\n",
        "        out = out.view(B, L, -1)\n",
        "        return self.out_projection(out), attn\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation=\"relu\"):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        d_ff = d_ff or 4 * d_model\n",
        "        self.attention = attention\n",
        "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        new_x, attn = self.attention(x, x, x, attn_mask=attn_mask)\n",
        "        x = x + self.dropout(new_x)\n",
        "        y = x = self.norm1(x)\n",
        "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
        "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
        "        return self.norm2(x + y), attn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.attn_layers = nn.ModuleList(attn_layers)\n",
        "        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
        "        self.norm = norm_layer\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        attns = []\n",
        "        if self.conv_layers is not None:\n",
        "            for i, (attn_layer, conv_layer) in enumerate(zip(self.attn_layers, self.conv_layers)):\n",
        "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
        "                x = conv_layer(x)\n",
        "                attns.append(attn)\n",
        "            x, attn = self.attn_layers[-1](x)\n",
        "            attns.append(attn)\n",
        "        else:\n",
        "            for attn_layer in self.attn_layers:\n",
        "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
        "                attns.append(attn)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "        return x, attns\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
        "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
        "        pe = torch.zeros(max_len, d_model).float()\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        pe.require_grad = False\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pe[:, : x.size(1)]"
      ],
      "metadata": {
        "id": "3OJX5y_JTx75"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHorizonFinancialDataset(Dataset):\n",
        "    def __init__(self, time_series_path, text_path, input_window=5, forecast_horizon=1, max_len=390):\n",
        "        self.input_window = input_window\n",
        "        self.forecast_horizon = forecast_horizon\n",
        "        self.max_len = max_len\n",
        "\n",
        "        time_series_data = pd.read_csv(time_series_path)\n",
        "        print(\"Processing time series data...\")\n",
        "        for col in ['open', 'high', 'low', 'close', 'volume']:\n",
        "            time_series_data[col] = time_series_data[col].apply(\n",
        "                lambda x: self._process_list(x, max_len)\n",
        "            )\n",
        "\n",
        "        print(\"Loading text data...\")\n",
        "        text_data = pd.read_csv(text_path)\n",
        "        print(\"Merging datasets...\")\n",
        "        self.data = pd.merge(\n",
        "            time_series_data,\n",
        "            text_data,\n",
        "            on='date',\n",
        "            how='inner'\n",
        "        )\n",
        "\n",
        "        self.dates = self.data['date'].values\n",
        "        self._process_features()\n",
        "        self._create_forecast_targets()\n",
        "\n",
        "    def _process_list(self, x, max_len):\n",
        "        try:\n",
        "            if isinstance(x, str):\n",
        "                values = eval(x)\n",
        "            else:\n",
        "                values = x\n",
        "            values = np.array(values, dtype=np.float32)\n",
        "            if np.any(np.isnan(values)) or np.any(np.isinf(values)):\n",
        "                values = np.nan_to_num(values, 0)\n",
        "\n",
        "            if len(values) > max_len:\n",
        "                return values[:max_len]\n",
        "            elif len(values) < max_len:\n",
        "                padding = np.full(max_len - len(values), values[-1])\n",
        "                return np.concatenate([values, padding])\n",
        "            return values\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing value: {str(e)}\")\n",
        "            return np.zeros(max_len)\n",
        "\n",
        "    def _compute_feature_stats(self):\n",
        "        try:\n",
        "            self.feature_stats = {\n",
        "                'mean': np.nanmean(self.features[:, :4, :], axis=(0, 2)),\n",
        "                'std': np.nanstd(self.features[:, :4, :], axis=(0, 2)),\n",
        "                'volume_mean': np.nanmean(self.features[:, 4, :]),\n",
        "                'volume_std': np.nanstd(self.features[:, 4, :])\n",
        "            }\n",
        "            self.feature_stats['std'] = np.where(\n",
        "                self.feature_stats['std'] == 0,\n",
        "                1e-6,\n",
        "                self.feature_stats['std']\n",
        "            )\n",
        "            if self.feature_stats['volume_std'] == 0:\n",
        "                self.feature_stats['volume_std'] = 1e-6\n",
        "            self._normalize_features()\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error computing feature statistics: {str(e)}\")\n",
        "\n",
        "    def _normalize_features(self):\n",
        "        for i in range(4):\n",
        "            self.features[:, i, :] = (\n",
        "                (self.features[:, i, :] - self.feature_stats['mean'][i]) /\n",
        "                self.feature_stats['std'][i]\n",
        "            )\n",
        "        self.features[:, 4, :] = (\n",
        "            (np.log1p(self.features[:, 4, :]) - np.log1p(self.feature_stats['volume_mean'])) /\n",
        "            (self.feature_stats['volume_std'] + 1e-8)\n",
        "        )\n",
        "\n",
        "    def _process_features(self):\n",
        "        print(\"Processing features...\")\n",
        "        self.features = []\n",
        "        for _, row in self.data.iterrows():\n",
        "            try:\n",
        "                daily_features = np.stack([\n",
        "                    row['open'],\n",
        "                    row['high'],\n",
        "                    row['low'],\n",
        "                    row['close'],\n",
        "                    row['volume']\n",
        "                ])\n",
        "                self.features.append(daily_features)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing row: {str(e)}\")\n",
        "                daily_features = np.zeros((5, self.max_len))\n",
        "                self.features.append(daily_features)\n",
        "        self.features = np.array(self.features)\n",
        "        self._compute_feature_stats()\n",
        "\n",
        "    def _create_forecast_targets(self):\n",
        "        print(\"Creating forecast targets...\")\n",
        "        self.targets = []\n",
        "        self.valid_indices = []\n",
        "        for i in range(len(self.features) - self.input_window - self.forecast_horizon):\n",
        "            current_close = self.features[i + self.input_window - 1][3, -1]\n",
        "            future_idx = i + self.input_window\n",
        "            future_price = self.features[future_idx][3, -1]\n",
        "            target = 1 if future_price > current_close else 0\n",
        "            self.targets.append(target)\n",
        "            self.valid_indices.append(i)\n",
        "        self.targets = np.array(self.targets)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        real_idx = self.valid_indices[idx]\n",
        "        x = self.features[real_idx:real_idx + self.input_window]\n",
        "        # text is ignored in baseline, but we must still return something\n",
        "        text_window = self.data['text'].iloc[real_idx:real_idx + self.input_window].tolist()\n",
        "        target = self.targets[idx]\n",
        "        dates = self.data['date'].iloc[real_idx:real_idx + self.input_window + self.forecast_horizon].tolist()\n",
        "        return {\n",
        "            \"x_enc\": torch.tensor(x, dtype=torch.float32),\n",
        "            \"text\": text_window,  # Ignored by model\n",
        "            \"targets\": torch.tensor(target, dtype=torch.long),\n",
        "            \"dates\": dates\n",
        "        }\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    x_enc = torch.stack([item['x_enc'] for item in batch])\n",
        "    targets = torch.tensor([item['targets'] for item in batch], dtype=torch.long)\n",
        "    dates = [item['dates'] for item in batch]\n",
        "    text = [item['text'] for item in batch]  # not used in baseline model\n",
        "    return {\n",
        "        'x_enc': x_enc,\n",
        "        'text': text,\n",
        "        'targets': targets,\n",
        "        'dates': dates\n",
        "    }"
      ],
      "metadata": {
        "id": "0QufaPoMT3GQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, patch_len, stride, padding, dropout):\n",
        "        super().__init__()\n",
        "        self.padding_patch_layer = nn.ReplicationPad1d((0, padding))\n",
        "        self.value_embedding = nn.Linear(patch_len * 5, d_model, bias=False)\n",
        "        self.position_embedding = PositionalEmbedding(d_model, max_len=1024)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.patch_len = patch_len\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, window_size, n_features, n_minutes = x.shape\n",
        "        x = x.reshape(-1, n_features, n_minutes)\n",
        "        x = self.padding_patch_layer(x)\n",
        "        x = x.unfold(dimension=-1, size=self.patch_len, step=self.stride)\n",
        "        num_patches = x.size(2)\n",
        "        x = x.permute(0, 2, 1, 3)\n",
        "        x = x.reshape(batch_size * window_size, num_patches, -1)\n",
        "        x = self.value_embedding(x)\n",
        "        x = x + self.position_embedding(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x.reshape(batch_size, window_size, num_patches, -1)\n",
        "        return x, n_features\n",
        "\n",
        "class PatchTSTBaseline(nn.Module):\n",
        "    def __init__(self, config, dataset):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.model_config = config['models']['patchtst']\n",
        "\n",
        "        self.enc_in = dataset.features.shape[1]  # 5\n",
        "        self.num_class = 2\n",
        "        self.max_seq_len = dataset.features.shape[2]\n",
        "        self.forecast_horizon = config['forecast_horizon']\n",
        "        self.input_window = config['input_window']\n",
        "\n",
        "        patch_len = self.model_config['patching']['patch_len']\n",
        "        stride = self.model_config['patching']['stride']\n",
        "        padding = self.model_config['patching']['padding']\n",
        "        self.n_patches = ((self.max_seq_len + padding) - patch_len) // stride + 1\n",
        "        self.projection_dim = self.input_window * self.n_patches * self.model_config['d_model']\n",
        "\n",
        "        print(f\"Input features shape: {dataset.features.shape}\")\n",
        "        print(f\"Patch length: {patch_len}\")\n",
        "        print(f\"Stride: {stride}\")\n",
        "        print(f\"Number of patches: {self.n_patches}\")\n",
        "        print(f\"Final projection dim: {self.projection_dim}\")\n",
        "\n",
        "        self.patch_embedding = PatchEmbedding(\n",
        "            d_model=self.model_config['d_model'],\n",
        "            patch_len=patch_len,\n",
        "            stride=stride,\n",
        "            padding=padding,\n",
        "            dropout=config['training']['dropout'],\n",
        "        )\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            [\n",
        "                EncoderLayer(\n",
        "                    AttentionLayer(\n",
        "                        FullAttention(False, factor=3, attention_dropout=config['training']['dropout']),\n",
        "                        self.model_config['d_model'],\n",
        "                        self.model_config['n_heads'],\n",
        "                    ),\n",
        "                    self.model_config['d_model'],\n",
        "                    self.model_config['d_ff'],\n",
        "                    dropout=config['training']['dropout'],\n",
        "                    activation=\"gelu\",\n",
        "                )\n",
        "                for _ in range(self.model_config['e_layers'])\n",
        "            ],\n",
        "            norm_layer=nn.LayerNorm(self.model_config['d_model']),\n",
        "        )\n",
        "\n",
        "        self.prediction_head = nn.Sequential(\n",
        "            nn.Linear(self.projection_dim, self.model_config['d_model']),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config['training']['dropout']),\n",
        "            nn.Linear(self.model_config['d_model'], self.num_class)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, W, 5, max_len]\n",
        "        B, W, n_features, n_minutes = x.shape\n",
        "        x_patched, _ = self.patch_embedding(x)  # [B, W, P, D]\n",
        "\n",
        "        B, W, P, D = x_patched.shape\n",
        "        x_reshaped = x_patched.reshape(B * W, P, D)\n",
        "        encoded_output, _ = self.encoder(x_reshaped)\n",
        "        encoded_output = encoded_output.reshape(B, W, P, D)\n",
        "\n",
        "        x_encoded = encoded_output.reshape(B, W * P * D)\n",
        "        output = self.prediction_head(x_encoded)  # [B, 2]\n",
        "        return output"
      ],
      "metadata": {
        "id": "wTQ17fm0T9dj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MetricTracker:\n",
        "    def __init__(self):\n",
        "        self.metrics = defaultdict(list)\n",
        "\n",
        "    def update(self, metrics_dict):\n",
        "        for k, v in metrics_dict.items():\n",
        "            self.metrics[k].append(v)\n",
        "\n",
        "    def get_latest(self, metric_name):\n",
        "        return self.metrics[metric_name][-1]\n",
        "\n",
        "    def get_history(self, metric_name):\n",
        "        return self.metrics[metric_name]\n",
        "\n",
        "class SingleHorizonTrainer:\n",
        "    def __init__(self, model, train_loader, val_loader, config):\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.config = config\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = torch.optim.Adam(\n",
        "            model.parameters(),\n",
        "            lr=config['lr'],\n",
        "            weight_decay=config['weight_decay']\n",
        "        )\n",
        "\n",
        "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer,\n",
        "            mode='min',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "    def train_epoch(self, epoch):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in tqdm.tqdm(self.train_loader):\n",
        "            self.optimizer.zero_grad()\n",
        "            x_enc = batch['x_enc'].to(self.device)\n",
        "            targets = batch['targets'].to(self.device)\n",
        "\n",
        "            outputs = self.model(x_enc)\n",
        "            loss = self.criterion(outputs, targets)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config['training']['gradient_clip'])\n",
        "            self.optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(self.train_loader)\n",
        "        return {'loss': avg_loss}\n",
        "\n",
        "    def validate(self):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        all_predictions = []\n",
        "        all_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in self.val_loader:\n",
        "                x_enc = batch['x_enc'].to(self.device)\n",
        "                targets = batch['targets'].to(self.device)\n",
        "                outputs = self.model(x_enc)\n",
        "                loss = self.criterion(outputs, targets)\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                preds = outputs.argmax(dim=1).cpu().numpy()\n",
        "                all_predictions.extend(preds)\n",
        "                all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "        avg_loss = total_loss / len(self.val_loader)\n",
        "        accuracy = accuracy_score(all_targets, all_predictions)\n",
        "        f1 = f1_score(all_targets, all_predictions, average='weighted')\n",
        "\n",
        "        return {'loss': avg_loss, 'accuracy': accuracy, 'f1': f1, 'predictions': all_predictions, 'targets': all_targets}"
      ],
      "metadata": {
        "id": "48I7BZLaGly8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_temporal_split(dataset, val_ratio=0.2, window_size=5):\n",
        "    total_size = len(dataset)\n",
        "    split_idx = int((1 - val_ratio) * total_size)\n",
        "    split_idx = max(split_idx - window_size, 0)\n",
        "\n",
        "    train_indices = list(range(split_idx))\n",
        "    val_indices = list(range(split_idx + window_size, total_size))\n",
        "\n",
        "    train_dataset = Subset(dataset, train_indices)\n",
        "    val_dataset = Subset(dataset, val_indices)\n",
        "\n",
        "    train_start = dataset.dates[dataset.valid_indices[0]]\n",
        "    train_end = dataset.dates[dataset.valid_indices[split_idx]] if split_idx < len(dataset.valid_indices) else dataset.dates[dataset.valid_indices[-1]]\n",
        "    val_start = dataset.dates[dataset.valid_indices[split_idx + window_size]] if (split_idx + window_size) < len(dataset.valid_indices) else dataset.dates[dataset.valid_indices[-1]]\n",
        "    val_end = dataset.dates[dataset.valid_indices[-1]]\n",
        "\n",
        "    print(f\"Training period: {train_start} to {train_end}\")\n",
        "    print(f\"Validation period: {val_start} to {val_end}\")\n",
        "\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "def plot_single_horizon_results(predictions, targets):\n",
        "    fig, ax = plt.subplots(figsize=(15, 5))\n",
        "    ax.plot(targets, label='Actual (0=Down, 1=Up)', alpha=0.6)\n",
        "    ax.plot(predictions, label='Predicted (0=Down, 1=Up)', alpha=0.6)\n",
        "    ax.set_title('Single Horizon Predictions')\n",
        "    ax.set_xlabel('Time Step')\n",
        "    ax.set_ylabel('Direction')\n",
        "    ax.legend()\n",
        "    acc = accuracy_score(targets, predictions)\n",
        "    ax.text(0.02, 0.98, f'Accuracy: {acc:.4f}', transform=ax.transAxes, verticalalignment='top')\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def plot_interactive_predictions(predictions, targets):\n",
        "    correct = (predictions == targets)\n",
        "    accuracy = np.mean(correct)\n",
        "    timesteps = np.arange(len(predictions))\n",
        "    direction_map = {0: \"Down\", 1: \"Up\"}\n",
        "    pred_directions = [direction_map[p] for p in predictions]\n",
        "    target_directions = [direction_map[t] for t in targets]\n",
        "    colors = np.where(correct, 'green', 'red')\n",
        "\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=timesteps,\n",
        "        y=predictions,\n",
        "        mode='markers',\n",
        "        marker=dict(\n",
        "            size=12,\n",
        "            color=colors,\n",
        "            opacity=0.7,\n",
        "            line=dict(width=1, color='DarkSlateGrey')\n",
        "        ),\n",
        "        text=[\n",
        "            f\"Step: {ts}<br>Predicted: {pd}<br>Actual: {td}\"\n",
        "            for ts, pd, td in zip(timesteps, pred_directions, target_directions)\n",
        "        ],\n",
        "        hovertemplate=\"<b>%{text}</b><extra></extra>\",\n",
        "        name=\"Predictions\"\n",
        "    ))\n",
        "\n",
        "    fig.add_hline(y=0.5, line_color=\"gray\", opacity=0.3)\n",
        "    fig.update_layout(\n",
        "        title={\n",
        "            'text': f\"Stock Direction Predictions (Accuracy: {accuracy*100:.2f}%)\",\n",
        "            'y':0.9,\n",
        "            'x':0.5,\n",
        "            'xanchor':'center',\n",
        "            'yanchor':'top'\n",
        "        },\n",
        "        xaxis_title=\"Time Step\",\n",
        "        yaxis_title=\"Direction (0=Down, 1=Up)\",\n",
        "        template=\"plotly_dark\"\n",
        "    )\n",
        "    return fig"
      ],
      "metadata": {
        "id": "KUoreN-Ha8Jm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    import torch\n",
        "    import logging\n",
        "    from torch.utils.data import DataLoader\n",
        "    import numpy as np\n",
        "    from pathlib import Path\n",
        "\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "        handlers=[\n",
        "            logging.FileHandler('forecast_training.log'),\n",
        "            logging.StreamHandler()\n",
        "        ]\n",
        "    )\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    config = {\n",
        "        'models': {\n",
        "            'patchtst': {\n",
        "                'd_model': 128,\n",
        "                'patching': {\n",
        "                    'patch_len': 30,\n",
        "                    'stride': 15,\n",
        "                    'padding': 15\n",
        "                },\n",
        "                'n_heads': 4,\n",
        "                'd_ff': 512,\n",
        "                'e_layers': 3,\n",
        "            }\n",
        "        },\n",
        "        'training': {\n",
        "            'dropout': 0.1,\n",
        "            'batch_size': 16,\n",
        "            'epochs': 20,\n",
        "            'gradient_clip': 1.0,\n",
        "            'early_stopping_patience': 5,\n",
        "            'validation_split': 0.2,\n",
        "        },\n",
        "        'forecast_horizon': 1,\n",
        "        'input_window': 5,\n",
        "        'lr': 1e-4,\n",
        "        'weight_decay': 1e-4\n",
        "    }\n",
        "\n",
        "    os.makedirs('checkpoints', exist_ok=True)\n",
        "    os.makedirs('results', exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        logger.info(\"Creating dataset...\")\n",
        "        dataset = MultiHorizonFinancialDataset(\n",
        "            time_series_path='AAPL_train_data_aggregated.csv',\n",
        "            text_path='AAPL_tweets_train.csv',\n",
        "            input_window=config['input_window'],\n",
        "            forecast_horizon=config['forecast_horizon']\n",
        "        )\n",
        "\n",
        "        logger.info(\"Creating train/val split using temporal split...\")\n",
        "        train_dataset, val_dataset = create_temporal_split(\n",
        "            dataset,\n",
        "            val_ratio=config['training']['validation_split'],\n",
        "            window_size=config['input_window']\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=config['training']['batch_size'],\n",
        "            shuffle=True,\n",
        "            collate_fn=custom_collate_fn,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=config['training']['batch_size'],\n",
        "            shuffle=False,\n",
        "            collate_fn=custom_collate_fn,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        logger.info(\"Initializing model...\")\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = PatchTSTBaseline(config=config, dataset=dataset).to(device)\n",
        "\n",
        "        trainer = SingleHorizonTrainer(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            config=config\n",
        "        )\n",
        "\n",
        "        logger.info(\"Starting training...\")\n",
        "        best_val_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "\n",
        "        for epoch in range(config['training']['epochs']):\n",
        "            train_metrics = trainer.train_epoch(epoch)\n",
        "            logger.info(f\"Epoch {epoch + 1}/{config['training']['epochs']}\")\n",
        "            logger.info(f\"Train Loss: {train_metrics['loss']:.4f}\")\n",
        "\n",
        "            val_metrics = trainer.validate()\n",
        "            logger.info(f\"Validation Loss: {val_metrics['loss']:.4f}\")\n",
        "            logger.info(f\"Validation Accuracy: {val_metrics['accuracy']:.4f}, Validation F1: {val_metrics['f1']:.4f}\")\n",
        "\n",
        "            train_losses.append(train_metrics['loss'])\n",
        "            val_losses.append(val_metrics['loss'])\n",
        "\n",
        "            trainer.scheduler.step(val_metrics['loss'])\n",
        "\n",
        "            if val_metrics['loss'] < best_val_loss:\n",
        "                best_val_loss = val_metrics['loss']\n",
        "                patience_counter = 0\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
        "                    'val_loss': val_metrics['loss'],\n",
        "                    'config': config,\n",
        "                }, 'checkpoints/best_model.pth')\n",
        "                logger.info(f\"Saved new best model with val_loss: {val_metrics['loss']:.4f}\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= config['training']['early_stopping_patience']:\n",
        "                    logger.info(\"Early stopping triggered\")\n",
        "                    break\n",
        "\n",
        "        logger.info(\"Performing final evaluation with best model...\")\n",
        "        checkpoint = torch.load('checkpoints/best_model.pth', map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        model.eval()\n",
        "\n",
        "        final_predictions = []\n",
        "        final_targets = []\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                x_enc = batch['x_enc'].to(device)\n",
        "                targets = batch['targets']\n",
        "                outputs = model(x_enc)\n",
        "                preds = outputs.argmax(dim=1).cpu().numpy()\n",
        "                final_predictions.extend(preds)\n",
        "                final_targets.extend(targets.numpy())\n",
        "\n",
        "        final_predictions = np.array(final_predictions)\n",
        "        final_targets = np.array(final_targets)\n",
        "        final_acc = accuracy_score(final_targets, final_predictions)\n",
        "        final_f1 = f1_score(final_targets, final_predictions)\n",
        "\n",
        "        final_metrics = {\n",
        "            'accuracy': float(final_acc),\n",
        "            'f1': float(final_f1)\n",
        "        }\n",
        "\n",
        "        with open('results/final_metrics.json', 'w') as f:\n",
        "            json.dump(final_metrics, f, indent=4)\n",
        "\n",
        "        # Plot and save train/val loss\n",
        "        plt.figure(figsize=(10,6))\n",
        "        plt.plot(train_losses, label='Train Loss', marker='o')\n",
        "        plt.plot(val_losses, label='Validation Loss', marker='o')\n",
        "        plt.title(\"Training and Validation Loss\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('results/train_val_loss.png')\n",
        "        plt.close()\n",
        "\n",
        "        # Confusion Matrix\n",
        "        cm = confusion_matrix(final_targets, final_predictions)\n",
        "        plt.figure(figsize=(6,5))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "        plt.title(\"Confusion Matrix\")\n",
        "        plt.xlabel(\"Predicted\")\n",
        "        plt.ylabel(\"Actual\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('results/confusion_matrix.png')\n",
        "        plt.close()\n",
        "\n",
        "        # Plot and save final interactive predictions (optional)\n",
        "        interactive_fig = plot_interactive_predictions(final_predictions, final_targets)\n",
        "        interactive_fig.write_html('results/final_forecast_interactive.html')\n",
        "        interactive_fig.write_image('results/final_forecast_interactive.png')\n",
        "\n",
        "        # Static plot of predictions\n",
        "        fig = plot_single_horizon_results(final_predictions, final_targets)\n",
        "        plt.savefig('results/final_forecast_results.png')\n",
        "        plt.close()\n",
        "\n",
        "        logger.info(\"Training completed successfully and results saved!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during training: {str(e)}\")\n",
        "        import traceback\n",
        "        logger.error(traceback.format_exc())\n",
        "        raise"
      ],
      "metadata": {
        "id": "eKooWBCOUTWi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "123af01d-5206-4a53-802c-ad1fdd7f671a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing time series data...\n",
            "Loading text data...\n",
            "Merging datasets...\n",
            "Processing features...\n",
            "Creating forecast targets...\n",
            "Training period: 2020-01-02 to 2021-08-18\n",
            "Validation period: 2021-08-25 to 2022-01-21\n",
            "Input features shape: (525, 5, 390)\n",
            "Patch length: 30\n",
            "Stride: 15\n",
            "Number of patches: 26\n",
            "Final projection dim: 16640\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "100%|██████████| 26/26 [00:01<00:00, 23.03it/s]\n",
            "100%|██████████| 26/26 [00:00<00:00, 46.43it/s]\n",
            "100%|██████████| 26/26 [00:00<00:00, 44.99it/s]\n",
            "100%|██████████| 26/26 [00:00<00:00, 43.13it/s]\n",
            "100%|██████████| 26/26 [00:00<00:00, 37.72it/s]\n",
            "100%|██████████| 26/26 [00:00<00:00, 39.67it/s]\n",
            "100%|██████████| 26/26 [00:00<00:00, 49.76it/s]\n",
            "100%|██████████| 26/26 [00:00<00:00, 51.23it/s]\n",
            "100%|██████████| 26/26 [00:00<00:00, 55.00it/s]\n",
            "100%|██████████| 26/26 [00:00<00:00, 48.15it/s]\n",
            "<ipython-input-10-09d581c3804b>:133: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load('checkpoints/best_model.pth', map_location=device)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lkYOn0APIv9z"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}